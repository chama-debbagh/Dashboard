
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import json
import io
import sqlite3
from datetime import datetime
from typing import Optional, List, Tuple, Any, Dict
from pathlib import Path
from io import StringIO
import sys
import chardet
import openpyxl


class DataExtractor:
    """Classe pour extraire les donn√©es de diff√©rents formats de fichiers"""
    
    def __init__(self):
        self.supported_formats = ['csv', 'xlsx', 'xls', 'json', 'txt']
    
    def extract_data(self, uploaded_file) -> Optional[pd.DataFrame]:
        """
        Extrait les donn√©es du fichier upload√©
        
        Args:
            uploaded_file: Fichier upload√© via Streamlit
            
        Returns:
            pd.DataFrame ou None si erreur
        """
        try:
            file_extension = self._get_file_extension(uploaded_file.name)
            
            if file_extension not in self.supported_formats:
                st.error(f"Format de fichier non support√©: {file_extension}")
                return None
            
            # Dispatcher vers la m√©thode appropri√©e
            if file_extension in ['xlsx', 'xls']:
                return self._extract_from_excel(uploaded_file)
            elif file_extension == 'csv':
                return self._extract_from_csv(uploaded_file)
            elif file_extension == 'json':
                return self._extract_from_json(uploaded_file)
            elif file_extension == 'txt':
                return self._extract_from_txt(uploaded_file)
            
        except Exception as e:
            st.error(f"Erreur lors de l'extraction: {str(e)}")
            return None
    
    def _get_file_extension(self, filename: str) -> str:
        """Obtient l'extension du fichier"""
        return Path(filename).suffix.lower().lstrip('.')
    
    def _extract_from_excel(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier Excel"""
        try:
            # Lire le fichier Excel avec gestion des erreurs
            excel_file = pd.ExcelFile(uploaded_file)
            
            # Si plusieurs feuilles, demander √† l'utilisateur de choisir
            if len(excel_file.sheet_names) > 1:
                st.info(f"Le fichier contient {len(excel_file.sheet_names)} feuilles")
                selected_sheet = st.selectbox(
                    "S√©lectionnez la feuille √† importer:",
                    excel_file.sheet_names,
                    key="excel_sheet_selector"
                )
                df = pd.read_excel(uploaded_file, sheet_name=selected_sheet)
            else:
                df = pd.read_excel(uploaded_file, sheet_name=0)
            
            # Nettoyage initial
            df = self._clean_dataframe(df)
            
            st.success(f"Fichier Excel import√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
            return df
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation Excel: {str(e)}")
            return None
    
    def _extract_from_csv(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier CSV avec d√©tection automatique"""
        try:
            # Lire les premiers octets pour d√©tecter l'encodage
            raw_data = uploaded_file.read()
            uploaded_file.seek(0)  # Remettre le curseur au d√©but
            
            # D√©tecter l'encodage
            encoding_result = chardet.detect(raw_data)
            encoding = encoding_result['encoding'] if encoding_result['confidence'] > 0.7 else 'utf-8'
            
            st.info(f"üîç Encodage d√©tect√©: {encoding} (confiance: {encoding_result['confidence']:.2f})")
            
            # Essayer diff√©rents s√©parateurs et configurations
            separators = [',', ';', '\t', '|']
            best_df = None
            best_cols = 0
            
            for sep in separators:
                try:
                    uploaded_file.seek(0)
                    df_test = pd.read_csv(
                        uploaded_file, 
                        sep=sep, 
                        encoding=encoding,
                        low_memory=False,
                        na_values=['', 'NA', 'N/A', 'NULL', 'null', '#N/A']
                    )
                    
                    # Garder le DataFrame avec le plus de colonnes coh√©rentes
                    if len(df_test.columns) > best_cols and len(df_test.columns) > 1:
                        best_df = df_test
                        best_cols = len(df_test.columns)
                        best_sep = sep
                        
                except:
                    continue
            
            if best_df is None:
                st.error("Impossible de d√©terminer le format CSV")
                return None
            
            st.success(f"CSV import√© avec s√©parateur '{best_sep}': {best_df.shape[0]} lignes, {best_df.shape[1]} colonnes")
            
            # Nettoyage
            best_df = self._clean_dataframe(best_df)
            return best_df
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation CSV: {str(e)}")
            return None
    
    def _extract_from_json(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier JSON"""
        try:
            json_data = json.load(uploaded_file)
            
            # Gestion de diff√©rentes structures JSON
            if isinstance(json_data, list):
                df = pd.json_normalize(json_data)
            elif isinstance(json_data, dict):
                # Essayer de trouver une liste dans le dictionnaire
                for key, value in json_data.items():
                    if isinstance(value, list) and len(value) > 0:
                        df = pd.json_normalize(value)
                        st.info(f"Donn√©es extraites de la cl√©: '{key}'")
                        break
                else:
                    # Si pas de liste trouv√©e, normaliser le dictionnaire
                    df = pd.json_normalize([json_data])
            else:
                st.error("Structure JSON non support√©e")
                return None
            
            st.success(f"JSON import√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
            return self._clean_dataframe(df)
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation JSON: {str(e)}")
            return None
    
    def _extract_from_txt(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier texte"""
        try:
            # Lire le contenu du fichier
            content = uploaded_file.read().decode('utf-8')
            lines = content.strip().split('\n')
            
            if not lines:
                st.error("Fichier texte vide")
                return None
            
            # Essayer de d√©tecter un format tabulaire
            first_line = lines[0]
            
            # D√©tecter le s√©parateur le plus probable
            separators = ['\t', ',', ';', '|', ' ']
            best_sep = None
            max_cols = 0
            
            for sep in separators:
                cols = len(first_line.split(sep))
                if cols > max_cols:
                    max_cols = cols
                    best_sep = sep
            
            if max_cols < 2:
                # Traiter comme texte simple
                df = pd.DataFrame({'Contenu': lines})
                st.info("Fichier trait√© comme texte simple")
            else:
                # Traiter comme donn√©es tabulaires
                data = []
                headers = lines[0].split(best_sep)
                
                for line in lines[1:]:
                    if line.strip():
                        values = line.split(best_sep)
                        # Ajuster la longueur si n√©cessaire
                        while len(values) < len(headers):
                            values.append('')
                        data.append(values[:len(headers)])
                
                df = pd.DataFrame(data, columns=headers)
                st.info(f"Fichier trait√© comme donn√©es tabulaires (s√©parateur: '{best_sep}')")
            
            st.success(f"Fichier texte import√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
            return self._clean_dataframe(df)
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation du fichier texte: {str(e)}")
            return None
    
    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Nettoie le DataFrame apr√®s importation"""
        try:
            # Supprimer les lignes enti√®rement vides
            df = df.dropna(how='all')
            
            # Supprimer les colonnes enti√®rement vides
            df = df.dropna(axis=1, how='all')
            
            # Nettoyer les noms de colonnes
            df.columns = df.columns.astype(str)
            df.columns = [col.strip() for col in df.columns]
            
            # Remplacer les noms de colonnes vides
            df.columns = [f'Colonne_{i}' if col == '' or col.startswith('Unnamed') 
                         else col for i, col in enumerate(df.columns)]
            
            # Supprimer les doublons de noms de colonnes
            df.columns = pd.io.common.dedup_names(df.columns, is_potential_multiindex=False)
            
            # Tentative de conversion automatique des types
            df = self._auto_convert_types(df)
            
            return df
            
        except Exception as e:
            st.warning(f"Erreur lors du nettoyage: {str(e)}")
            return df
    
    def _auto_convert_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """Conversion automatique des types de donn√©es"""
        try:
            for col in df.columns:
                # Essayer de convertir en num√©rique
                numeric_col = pd.to_numeric(df[col], errors='coerce')
                if not numeric_col.isna().all():
                    # Si plus de 80% des valeurs sont num√©riques, convertir
                    valid_numeric = (~numeric_col.isna()).sum()
                    total_non_null = (~df[col].isna()).sum()
                    
                    if total_non_null > 0 and (valid_numeric / total_non_null) > 0.8:
                        df[col] = numeric_col
                        continue
                
                # Essayer de convertir en datetime
                try:
                    datetime_col = pd.to_datetime(df[col], errors='coerce')
                    valid_datetime = (~datetime_col.isna()).sum()
                    total_non_null = (~df[col].isna()).sum()
                    
                    if total_non_null > 0 and (valid_datetime / total_non_null) > 0.8:
                        df[col] = datetime_col
                        continue
                except:
                    pass
                
                # Nettoyer les colonnes texte
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str).str.strip()
                    df[col] = df[col].replace('nan', pd.NA)
            
            return df
            
        except Exception as e:
            st.warning(f"Erreur lors de la conversion des types: {str(e)}")
            return df






class DataAnalyzer:
    """Classe pour analyser les donn√©es et g√©n√©rer des insights"""
    
    def __init__(self):
        pass
    
    def get_column_info(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Retourne des informations d√©taill√©es sur chaque colonne
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            DataFrame avec les informations des colonnes
        """
        info_data = []
        
        for col in df.columns:
            col_data = {
                'Colonne': col,
                'Type': str(df[col].dtype),
                'Valeurs_uniques': df[col].nunique(),
                'Valeurs_manquantes': df[col].isnull().sum(),
                'Pourcentage_manquant': f"{(df[col].isnull().sum() / len(df)) * 100:.1f}%",
                'Taille_m√©moire_KB': f"{df[col].memory_usage(deep=True) / 1024:.1f}"
            }
            
            # Ajouter des statistiques sp√©cifiques selon le type
            if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                col_data.update({
                    'Min': df[col].min() if not df[col].empty else None,
                    'Max': df[col].max() if not df[col].empty else None,
                    'Moyenne': f"{df[col].mean():.2f}" if not df[col].empty else None,
                    'M√©diane': f"{df[col].median():.2f}" if not df[col].empty else None
                })
            elif df[col].dtype == 'object':
                col_data.update({
                    'Longueur_min': df[col].astype(str).str.len().min() if not df[col].empty else None,
                    'Longueur_max': df[col].astype(str).str.len().max() if not df[col].empty else None,
                    'Longueur_moyenne': f"{df[col].astype(str).str.len().mean():.1f}" if not df[col].empty else None,
                    'Valeur_fr√©quente': df[col].mode().iloc[0] if not df[col].mode().empty else None
                })
            
            info_data.append(col_data)
        
        return pd.DataFrame(info_data)
    
    def get_categorical_stats(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Statistiques pour les colonnes cat√©gorielles
        
        Args:
            df: DataFrame avec colonnes cat√©gorielles
            
        Returns:
            DataFrame avec statistiques cat√©gorielles
        """
        stats_data = []
        
        for col in df.columns:
            if df[col].nunique() < 50:  # Seulement pour colonnes avec peu de valeurs uniques
                value_counts = df[col].value_counts()
                stats_data.append({
                    'Colonne': col,
                    'Valeurs_uniques': df[col].nunique(),
                    'Valeur_dominante': value_counts.index[0] if len(value_counts) > 0 else None,
                    'Fr√©quence_dominante': value_counts.iloc[0] if len(value_counts) > 0 else 0,
                    'Pourcentage_dominante': f"{(value_counts.iloc[0] / len(df)) * 100:.1f}%" if len(value_counts) > 0 else "0%",
                    'Entropie': self._calculate_entropy(df[col])
                })
        
        return pd.DataFrame(stats_data)
    
    def analyze_data_quality(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Analyse la qualit√© des donn√©es
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            DataFrame avec rapport de qualit√©
        """
        quality_data = []
        
        for col in df.columns:
            # Calculs de base
            total_rows = len(df)
            missing_count = df[col].isnull().sum()
            missing_pct = (missing_count / total_rows) * 100
            unique_count = df[col].nunique()
            unique_pct = (unique_count / total_rows) * 100
            
            # Score de qualit√© (0-100)
            quality_score = 100
            if missing_pct > 50:
                quality_score -= 30
            elif missing_pct > 20:
                quality_score -= 15
            elif missing_pct > 5:
                quality_score -= 5
            
            # Probl√®mes potentiels
            issues = []
            if missing_pct > 20:
                issues.append("Beaucoup de valeurs manquantes")
            if unique_pct < 1 and df[col].dtype in ['object']:
                issues.append("Peu de diversit√©")
            if unique_count == 1:
                issues.append("Valeur constante")
            if df[col].dtype == 'object':
                # V√©rifier la coh√©rence des formats
                sample_values = df[col].dropna().astype(str).str.strip()
                if len(sample_values) > 0:
                    lengths = sample_values.str.len()
                    if lengths.std() > lengths.mean():
                        issues.append("Formats incoh√©rents")
            
            quality_data.append({
                'Colonne': col,
                'Score_qualit√©': f"{quality_score:.0f}/100",
                'Valeurs_manquantes': f"{missing_count} ({missing_pct:.1f}%)",
                'Valeurs_uniques': f"{unique_count} ({unique_pct:.1f}%)",
                'Probl√®mes': "; ".join(issues) if issues else "Aucun",
                'Recommandation': self._get_quality_recommendation(missing_pct, unique_pct, issues)
            })
        
        return pd.DataFrame(quality_data)
    
    def generate_insights(self, df: pd.DataFrame) -> List[str]:
        """
        G√©n√®re des insights automatiques sur les donn√©es
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            Liste d'insights
        """
        insights = []
        
        # Insights g√©n√©raux
        total_rows = len(df)
        total_cols = len(df.columns)
        
        insights.append(f"Le dataset contient {total_rows:,} lignes et {total_cols} colonnes")
        
        # Insights sur les valeurs manquantes
        missing_total = df.isnull().sum().sum()
        missing_pct = (missing_total / (total_rows * total_cols)) * 100
        if missing_pct > 10:
            insights.append(f"Attention: {missing_pct:.1f}% des donn√©es sont manquantes")
        elif missing_pct == 0:
            insights.append("Excellent: Aucune valeur manquante d√©tect√©e")
        
        # Insights sur les types de donn√©es
        numeric_cols = len(df.select_dtypes(include=['number']).columns)
        text_cols = len(df.select_dtypes(include=['object']).columns)
        date_cols = len(df.select_dtypes(include=['datetime']).columns)
        
        if numeric_cols > text_cols:
            insights.append(f"Dataset majoritairement num√©rique ({numeric_cols} colonnes num√©riques vs {text_cols} textuelles)")
        elif text_cols > numeric_cols:
            insights.append(f"Dataset majoritairement textuel ({text_cols} colonnes textuelles vs {numeric_cols} num√©riques)")
        
        if date_cols > 0:
            insights.append(f"Dataset temporel d√©tect√© avec {date_cols} colonne(s) de dates")
        
        # Insights sur la distribution
        for col in df.select_dtypes(include=['number']).columns[:3]:  # Top 3 colonnes num√©riques
            skewness = df[col].skew()
            if abs(skewness) > 2:
                skew_type = "tr√®s asym√©trique √† droite" if skewness > 0 else "tr√®s asym√©trique √† gauche"
                insights.append(f"La colonne '{col}' a une distribution {skew_type}")
        
        # Insights sur les corr√©lations
        numeric_df = df.select_dtypes(include=['number'])
        if len(numeric_df.columns) > 1:
            corr_matrix = numeric_df.corr()
            # Trouver les corr√©lations les plus fortes (hors diagonale)
            corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.7:
                        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))
            
            if corr_pairs:
                best_corr = max(corr_pairs, key=lambda x: abs(x[2]))
                insights.append(f"Forte corr√©lation d√©tect√©e entre '{best_corr[0]}' et '{best_corr[1]}' (r={best_corr[2]:.2f})")
        
        # Insights sur les outliers
        for col in df.select_dtypes(include=['number']).columns[:2]:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
            
            if len(outliers) > 0:
                outlier_pct = (len(outliers) / len(df)) * 100
                insights.append(f"La colonne '{col}' contient {len(outliers)} valeurs aberrantes ({outlier_pct:.1f}%)")
        
        return insights[:8]  # Limiter √† 8 insights
    
    def get_recommendations(self, df: pd.DataFrame) -> List[str]:
        """
        G√©n√®re des recommandations pour am√©liorer les donn√©es
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            Liste de recommandations
        """
        recommendations = []
        
        # Recommandations sur les valeurs manquantes
        high_missing_cols = []
        for col in df.columns:
            missing_pct = (df[col].isnull().sum() / len(df)) * 100
            if missing_pct > 20:
                high_missing_cols.append((col, missing_pct))
        
        if high_missing_cols:
            recommendations.append(f"Traiter les valeurs manquantes dans {len(high_missing_cols)} colonne(s): " + 
                                 ", ".join([f"{col} ({pct:.1f}%)" for col, pct in high_missing_cols[:3]]))
        
        # Recommandations sur les doublons
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            recommendations.append(f"Supprimer {duplicates} ligne(s) dupliqu√©e(s)")
        
        # Recommandations sur les types de donn√©es
        for col in df.select_dtypes(include=['object']).columns:
            # V√©rifier si la colonne pourrait √™tre num√©rique
            try:
                numeric_converted = pd.to_numeric(df[col], errors='coerce')
                non_null_original = df[col].notna().sum()
                non_null_converted = numeric_converted.notna().sum()
                
                if non_null_converted / non_null_original > 0.8:
                    recommendations.append(f"Convertir la colonne '{col}' en type num√©rique")
            except:
                pass
            
            # V√©rifier si la colonne pourrait √™tre cat√©gorielle
            if df[col].nunique() < 20 and df[col].nunique() / len(df) < 0.1:
                recommendations.append(f"Convertir la colonne '{col}' en type cat√©goriel pour optimiser la m√©moire")
        
        # Recommandations sur la normalisation
        numeric_cols = df.select_dtypes(include=['number']).columns
        for col in numeric_cols:
            if df[col].std() > 0:
                col_range = df[col].max() - df[col].min()
                col_mean = df[col].mean()
                if col_range > 1000 or col_mean > 1000:
                    recommendations.append(f"Consid√©rer la normalisation de la colonne '{col}' pour les analyses")
        
        # Recommandations sur l'indexation
        if len(df) > 10000:
            recommendations.append("Consid√©rer l'ajout d'un index pour am√©liorer les performances sur ce large dataset")
        
        # Recommandations sur les visualisations
        if len(numeric_cols) >= 2:
            recommendations.append("Cr√©er des graphiques de corr√©lation pour explorer les relations entre variables")
        
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            recommendations.append("Analyser la distribution des variables cat√©gorielles avec des graphiques en barres")
        
        return recommendations[:6]  # Limiter √† 6 recommandations
    
    def _calculate_entropy(self, series: pd.Series) -> float:
        """Calcule l'entropie d'une s√©rie (mesure de diversit√©)"""
        try:
            value_counts = series.value_counts()
            probabilities = value_counts / len(series)
            entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
            return round(entropy, 3)
        except:
            return 0.0
    
    def _get_quality_recommendation(self, missing_pct: float, unique_pct: float, issues: List[str]) -> str:
        """G√©n√®re une recommandation bas√©e sur la qualit√© de la colonne"""
        if missing_pct > 50:
            return "Consid√©rer la suppression de cette colonne"
        elif missing_pct > 20:
            return "Imputer les valeurs manquantes"
        elif "Valeur constante" in issues:
            return "Supprimer cette colonne (pas d'information)"
        elif "Formats incoh√©rents" in issues:
            return "Standardiser le format des donn√©es"
        elif unique_pct < 1:
            return "V√©rifier la coh√©rence des donn√©es"
        else:
            return "Colonne de bonne qualit√©"




class DataVisualizer:
    """Classe pour cr√©er des visualisations automatiques des donn√©es"""
    
    def __init__(self):
        # Palette de couleurs moderne
        self.color_palette = [
            '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
            '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'
        ]
        
        # Template de style pour les graphiques
        self.layout_template = {
            'font': {'family': 'Arial, sans-serif', 'size': 12},
            'title': {'x': 0.5, 'xanchor': 'center'},
            'plot_bgcolor': 'white',
            'paper_bgcolor': 'white',
            'margin': {'l': 60, 'r': 60, 't': 80, 'b': 60}
        }
    
    def auto_generate_charts(self, df: pd.DataFrame) -> List[Tuple[str, str, Any]]:
        """
        G√©n√®re automatiquement des graphiques appropri√©s selon les donn√©es
        
        Args:
            df: DataFrame √† visualiser
            
        Returns:
            Liste de tuples (type_graphique, nom, figure_plotly)
        """
        charts = []
        
        # S√©parer les colonnes par type
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime']).columns.tolist()
        
        # 1. Histogrammes pour colonnes num√©riques
        for col in numeric_cols[:4]:  # Limiter √† 4 pour √©viter la surcharge
            fig = self._create_histogram(df, col)
            charts.append(("histogram", f"Distribution de {col}", fig))
        
        # 2. Graphiques en barres pour colonnes cat√©gorielles
        for col in categorical_cols[:3]:
            if df[col].nunique() <= 15:  # Seulement si peu de cat√©gories
                fig = self._create_bar_chart(df, col)
                charts.append(("bar", f"R√©partition de {col}", fig))
        
        # 3. Scatter plots pour paires de variables num√©riques
        if len(numeric_cols) >= 2:
            # Cr√©er scatter plot pour les 2 premi√®res colonnes num√©riques
            fig = self._create_scatter_plot(df, numeric_cols[0], numeric_cols[1])
            charts.append(("scatter", f"{numeric_cols[0]} vs {numeric_cols[1]}", fig))
            
            # Si plus de 2 colonnes num√©riques, ajouter un autre scatter plot
            if len(numeric_cols) >= 3:
                fig = self._create_scatter_plot(df, numeric_cols[0], numeric_cols[2])
                charts.append(("scatter", f"{numeric_cols[0]} vs {numeric_cols[2]}", fig))
        
        # 4. Box plots pour distribution par cat√©gorie
        if len(numeric_cols) >= 1 and len(categorical_cols) >= 1:
            # Prendre la premi√®re colonne cat√©gorielle avec peu de valeurs uniques
            cat_col = None
            for col in categorical_cols:
                if df[col].nunique() <= 10:
                    cat_col = col
                    break
            
            if cat_col:
                fig = self._create_box_plot(df, numeric_cols[0], cat_col)
                charts.append(("box", f"{numeric_cols[0]} par {cat_col}", fig))
        
        # 5. Graphiques temporels si colonnes datetime
        if len(datetime_cols) >= 1 and len(numeric_cols) >= 1:
            fig = self._create_time_series(df, datetime_cols[0], numeric_cols[0])
            charts.append(("timeseries", f"√âvolution de {numeric_cols[0]}", fig))
        
        # 6. Heatmap de corr√©lation si suffisamment de colonnes num√©riques
        if len(numeric_cols) >= 3:
            fig = self.create_correlation_matrix(df[numeric_cols])
            charts.append(("heatmap", "Matrice de corr√©lation", fig))
        
        return charts
    
    def _create_histogram(self, df: pd.DataFrame, column: str) -> go.Figure:
        """Cr√©e un histogramme pour une colonne num√©rique"""
        fig = px.histogram(
            df, 
            x=column,
            nbins=30,
            title=f"Distribution de {column}",
            color_discrete_sequence=[self.color_palette[0]]
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=column,
            yaxis_title="Fr√©quence",
            showlegend=False
        )
        
        # Ajouter ligne de moyenne
        mean_val = df[column].mean()
        fig.add_vline(
            x=mean_val, 
            line_dash="dash", 
            line_color="red",
            annotation_text=f"Moyenne: {mean_val:.2f}"
        )
        
        return fig
    
    def _create_bar_chart(self, df: pd.DataFrame, column: str) -> go.Figure:
        """Cr√©e un graphique en barres pour une colonne cat√©gorielle"""
        value_counts = df[column].value_counts().head(10)  # Top 10
        
        fig = px.bar(
            x=value_counts.index,
            y=value_counts.values,
            title=f"R√©partition de {column}",
            color=value_counts.values,
            color_continuous_scale="viridis"
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=column,
            yaxis_title="Fr√©quence",
            showlegend=False
        )
        
        # Rotation des labels si n√©cessaires
        if max([len(str(x)) for x in value_counts.index]) > 10:
            fig.update_xaxes(tickangle=45)
        
        return fig
    
    def _create_scatter_plot(self, df: pd.DataFrame, x_col: str, y_col: str) -> go.Figure:
        """Cr√©e un scatter plot entre deux colonnes num√©riques"""
        fig = px.scatter(
            df,
            x=x_col,
            y=y_col,
            title=f"Relation entre {x_col} et {y_col}",
            color_discrete_sequence=[self.color_palette[2]],
            opacity=0.7
        )
        
        # Ajouter ligne de tendance
        try:
            fig.add_traces(
                px.scatter(df, x=x_col, y=y_col, trendline="ols").data[1]
            )
        except:
            pass
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=x_col,
            yaxis_title=y_col
        )
        
        # Calculer et afficher la corr√©lation
        correlation = df[x_col].corr(df[y_col])
        fig.add_annotation(
            x=0.02, y=0.98,
            xref="paper", yref="paper",
            text=f"Corr√©lation: {correlation:.3f}",
            showarrow=False,
            bgcolor="white",
            bordercolor="black",
            borderwidth=1
        )
        
        return fig
    
    def _create_box_plot(self, df: pd.DataFrame, numeric_col: str, cat_col: str) -> go.Figure:
        """Cr√©e un box plot pour analyser la distribution d'une variable num√©rique par cat√©gorie"""
        fig = px.box(
            df,
            x=cat_col,
            y=numeric_col,
            title=f"Distribution de {numeric_col} par {cat_col}",
            color=cat_col
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=cat_col,
            yaxis_title=numeric_col
        )
        
        if df[cat_col].nunique() > 5:
            fig.update_xaxes(tickangle=45)
        
        return fig
    
    def _create_time_series(self, df: pd.DataFrame, date_col: str, value_col: str) -> go.Figure:
        """Cr√©e un graphique temporel"""
        # Trier par date
        df_sorted = df.sort_values(date_col)
        
        fig = px.line(
            df_sorted,
            x=date_col,
            y=value_col,
            title=f"√âvolution temporelle de {value_col}",
            color_discrete_sequence=[self.color_palette[1]]
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title="Date",
            yaxis_title=value_col
        )
        
        return fig
    
    def create_correlation_matrix(self, df: pd.DataFrame) -> go.Figure:
        """Cr√©e une heatmap de corr√©lation"""
        # Calculer la matrice de corr√©lation
        corr_matrix = df.corr()
        
        # Cr√©er la heatmap
        fig = px.imshow(
            corr_matrix,
            text_auto=True,
            aspect="auto",
            title="Matrice de corr√©lation",
            color_continuous_scale="RdBu_r",
            zmin=-1,
            zmax=1
        )
        
        fig.update_layout(
            **self.layout_template,
            width=600,
            height=500
        )
        
        return fig
    
    def create_missing_data_heatmap(self, df: pd.DataFrame) -> go.Figure:
        """Cr√©e une heatmap des valeurs manquantes"""
        # Cr√©er matrice des valeurs manquantes
        missing_data = df.isnull().astype(int)
        
        fig = px.imshow(
            missing_data.T,  # Transposer pour avoir colonnes en y
            title="Carte des valeurs manquantes (blanc = manquant)",
            color_continuous_scale=["white", "red"],
            aspect="auto"
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title="Index des lignes",
            yaxis_title="Colonnes",
            height=400
        )
        
        return fig
    
    def create_distribution_comparison(self, df: pd.DataFrame, columns: List[str]) -> go.Figure:
        """Compare la distribution de plusieurs colonnes num√©riques"""
        fig = make_subplots(
            rows=1, cols=len(columns),
            subplot_titles=columns,
            shared_yaxes=True
        )
        
        for i, col in enumerate(columns):
            fig.add_trace(
                go.Histogram(
                    x=df[col],
                    name=col,
                    marker_color=self.color_palette[i % len(self.color_palette)],
                    opacity=0.7
                ),
                row=1, col=i+1
            )
        
        fig.update_layout(
            title="Comparaison des distributions",
            **self.layout_template,
            height=400,
            showlegend=False
        )
        
        return fig
    
    def create_statistical_summary_chart(self, df: pd.DataFrame) -> go.Figure:
        """Cr√©e un graphique r√©sum√© des statistiques"""
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        
        if len(numeric_cols) == 0:
            return None
        
        # Calculer les statistiques
        stats = df[numeric_cols].describe().T
        
        fig = go.Figure()
        
        # Ajouter les barres pour moyenne et m√©diane
        fig.add_trace(go.Bar(
            name='Moyenne',
            x=stats.index,
            y=stats['mean'],
            marker_color=self.color_palette[0]
        ))
        
        fig.add_trace(go.Bar(
            name='M√©diane',
            x=stats.index,
            y=stats['50%'],
            marker_color=self.color_palette[1]
        ))
        
        fig.update_layout(
            title="Comparaison Moyenne vs M√©diane",
            **self.layout_template,
            barmode='group',
            xaxis_title="Variables",
            yaxis_title="Valeurs"
        )
        
        return fig






class PowerBIExporter:
    """
    Classe CORRIG√âE pour exporter vers Power BI avec g√©n√©ration de template .pbit
    
    CHANGEMENTS MAJEURS:
    1. G√©n√©ration d'un vrai fichier .pbit (Power BI Template)
    2. Cr√©ation de la structure JSON conforme au format Power BI
    3. Ajout de visualisations pr√©-configur√©es
    4. G√©n√©ration de mesures DAX exploitables
    5. Configuration du mod√®le de donn√©es avec relations
    """
    
    def __init__(self):
        self.version = "2.118.828.0"  # Version Power BI compatible
    
    def create_powerbi_template(self, df: pd.DataFrame, filename: str) -> bytes:
        """
        FONCTION PRINCIPALE CORRIG√âE
        Cr√©e un fichier .pbit (Power BI Template) complet
        
        POURQUOI CE CHANGEMENT:
        - Un .pbit est un fichier ZIP contenant des fichiers JSON structur√©s
        - Il contient: Layout (visuels), DataModel (sch√©ma), et Metadata
        - Permet d'ouvrir directement dans Power BI Desktop
        
        Args:
            df: DataFrame source
            filename: Nom du fichier original
            
        Returns:
            bytes: Contenu du fichier .pbit
        """
        # Cr√©er un buffer m√©moire pour le ZIP
        zip_buffer = io.BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # 1. LAYOUT - D√©finit les pages et visualisations
            layout_json = self._create_layout_structure(df, filename)
            zip_file.writestr('Report/Layout', json.dumps(layout_json, indent=2))
            
            # 2. DATA MODEL - Structure du mod√®le de donn√©es
            datamodel_json = self._create_datamodel_structure(df, filename)
            zip_file.writestr('DataModelSchema', json.dumps(datamodel_json, indent=2))
            
            # 3. METADATA - Informations du template
            metadata_json = self._create_metadata()
            zip_file.writestr('Metadata', json.dumps(metadata_json, indent=2))
            
            # 4. VERSION - Version Power BI
            version_json = {"version": self.version}
            zip_file.writestr('Version', json.dumps(version_json))
            
            # 5. CONNECTIONS - Configuration connexion donn√©es
            connections_json = self._create_connections(filename)
            zip_file.writestr('Connections', json.dumps(connections_json, indent=2))
        
        zip_buffer.seek(0)
        return zip_buffer.getvalue()
    
    def _create_layout_structure(self, df: pd.DataFrame, filename: str) -> dict:
        """
        Cr√©e la structure Layout avec visualisations pr√©-configur√©es
        
        POURQUOI IMPORTANT:
        - D√©finit l'apparence du rapport
        - Contient les visuels (graphiques, tableaux, cartes)
        - Positionne les √©l√©ments sur la page
        """
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
        # Liste des visuels √† cr√©er
        visuals = []
        
        # VISUAL 1: Tableau de donn√©es (en haut √† gauche)
        if len(df.columns) > 0:
            visuals.append(self._create_table_visual(df.columns.tolist()[:5], 0, 0))
        
        # VISUAL 2: Carte avec KPI (si colonnes num√©riques)
        if numeric_cols:
            visuals.append(self._create_card_visual(numeric_cols[0], 600, 0))
        
        # VISUAL 3: Graphique en barres (si colonnes cat√©gorielles et num√©riques)
        if categorical_cols and numeric_cols:
            visuals.append(self._create_bar_chart_visual(
                categorical_cols[0], 
                numeric_cols[0], 
                0, 300
            ))
        
        # VISUAL 4: Graphique lin√©aire (si 2+ colonnes num√©riques)
        if len(numeric_cols) >= 2:
            visuals.append(self._create_line_chart_visual(
                numeric_cols[0],
                numeric_cols[1],
                600, 300
            ))
        
        layout = {
            "id": 0,
            "resourcePackages": [],
            "name": f"ReportSection_{filename}",
            "displayName": f"Analyse {filename}",
            "width": 1280,
            "height": 720,
            "displayOption": 1,
            "background": {
                "color": "#FFFFFF",
                "transparency": 100
            },
            "visualContainers": visuals,
            "filters": "[]",
            "ordinal": 0
        }
        
        return {
            "id": "1",
            "pages": [layout],
            "config": "{}"
        }
    
    def _create_table_visual(self, columns: list, x: int, y: int) -> dict:
        """
        Cr√©e un visuel de type tableau
        
        STRUCTURE:
        - Type: tableEx (tableau Power BI)
        - Position: x, y
        - Dimensions: width, height
        - Colonnes: liste des champs √† afficher
        """
        return {
            "x": x,
            "y": y,
            "z": 0,
            "width": 500,
            "height": 250,
            "config": json.dumps({
                "name": "table1",
                "layouts": [{
                    "id": 0,
                    "position": {
                        "x": x,
                        "y": y,
                        "z": 0,
                        "width": 500,
                        "height": 250
                    }
                }],
                "singleVisual": {
                    "visualType": "tableEx",
                    "projections": {
                        "Values": [{"queryRef": f"Sum({col})"} for col in columns]
                    },
                    "prototypeQuery": {
                        "Version": 2,
                        "From": [{"Name": "t", "Entity": "Table1"}]
                    }
                }
            })
        }
    
    def _create_card_visual(self, column: str, x: int, y: int) -> dict:
        """
        Cr√©e un visuel de type carte (KPI)
        
        UTILIT√â:
        - Affiche une m√©trique unique en grand
        - Parfait pour les KPIs importants
        """
        return {
            "x": x,
            "y": y,
            "z": 1,
            "width": 250,
            "height": 150,
            "config": json.dumps({
                "name": "card1",
                "layouts": [{
                    "id": 0,
                    "position": {
                        "x": x,
                        "y": y,
                        "z": 1,
                        "width": 250,
                        "height": 150
                    }
                }],
                "singleVisual": {
                    "visualType": "card",
                    "projections": {
                        "Values": [{"queryRef": f"Sum({column})"}]
                    },
                    "prototypeQuery": {
                        "Version": 2,
                        "From": [{"Name": "t", "Entity": "Table1"}],
                        "Select": [{
                            "Aggregation": {
                                "Expression": {"Column": {"Expression": {"SourceRef": {"Source": "t"}}, "Property": column}},
                                "Function": 0
                            },
                            "Name": f"Sum({column})"
                        }]
                    }
                }
            })
        }
    
    def _create_bar_chart_visual(self, category_col: str, value_col: str, x: int, y: int) -> dict:
        """
        Cr√©e un graphique en barres
        
        CONFIGURATION:
        - Axe X: cat√©gories
        - Axe Y: valeurs num√©riques
        - Type: barChart (clusteredBarChart)
        """
        return {
            "x": x,
            "y": y,
            "z": 2,
            "width": 550,
            "height": 350,
            "config": json.dumps({
                "name": "barChart1",
                "layouts": [{
                    "id": 0,
                    "position": {
                        "x": x,
                        "y": y,
                        "z": 2,
                        "width": 550,
                        "height": 350
                    }
                }],
                "singleVisual": {
                    "visualType": "clusteredBarChart",
                    "projections": {
                        "Category": [{"queryRef": category_col}],
                        "Values": [{"queryRef": f"Sum({value_col})"}]
                    },
                    "prototypeQuery": {
                        "Version": 2,
                        "From": [{"Name": "t", "Entity": "Table1"}],
                        "Select": [
                            {"Column": {"Expression": {"SourceRef": {"Source": "t"}}, "Property": category_col}},
                            {
                                "Aggregation": {
                                    "Expression": {"Column": {"Expression": {"SourceRef": {"Source": "t"}}, "Property": value_col}},
                                    "Function": 0
                                },
                                "Name": f"Sum({value_col})"
                            }
                        ]
                    }
                }
            })
        }
    
    def _create_line_chart_visual(self, x_col: str, y_col: str, x: int, y: int) -> dict:
        """
        Cr√©e un graphique lin√©aire
        
        USAGE:
        - Parfait pour tendances temporelles
        - Compare √©volutions de 2 variables
        """
        return {
            "x": x,
            "y": y,
            "z": 3,
            "width": 550,
            "height": 350,
            "config": json.dumps({
                "name": "lineChart1",
                "layouts": [{
                    "id": 0,
                    "position": {
                        "x": x,
                        "y": y,
                        "z": 3,
                        "width": 550,
                        "height": 350
                    }
                }],
                "singleVisual": {
                    "visualType": "lineChart",
                    "projections": {
                        "Category": [{"queryRef": x_col}],
                        "Values": [{"queryRef": f"Sum({y_col})"}]
                    },
                    "prototypeQuery": {
                        "Version": 2,
                        "From": [{"Name": "t", "Entity": "Table1"}],
                        "Select": [
                            {"Column": {"Expression": {"SourceRef": {"Source": "t"}}, "Property": x_col}},
                            {
                                "Aggregation": {
                                    "Expression": {"Column": {"Expression": {"SourceRef": {"Source": "t"}}, "Property": y_col}},
                                    "Function": 0
                                },
                                "Name": f"Sum({y_col})"
                            }
                        ]
                    }
                }
            })
        }
    
    def _create_datamodel_structure(self, df: pd.DataFrame, filename: str) -> dict:
        """
        Cr√©e le sch√©ma du mod√®le de donn√©es
        
        R√îLE CRUCIAL:
        - D√©finit les tables
        - Sp√©cifie les colonnes et types
        - Configure les relations entre tables
        - D√©finit les mesures DAX
        """
        columns = []
        measures = []
        
        # D√©finir chaque colonne avec son type
        for col in df.columns:
            dtype = df[col].dtype
            
            # Mapper les types pandas vers types Power BI
            if dtype in ['int64', 'int32', 'float64', 'float32']:
                col_type = "Int64"  # Type num√©rique Power BI
                
                # Cr√©er des mesures DAX automatiques pour colonnes num√©riques
                measures.extend([
                    {
                        "name": f"{col}_Total",
                        "expression": f"SUM(Table1[{col}])",
                        "formatString": "#,##0.00"
                    },
                    {
                        "name": f"{col}_Moyenne",
                        "expression": f"AVERAGE(Table1[{col}])",
                        "formatString": "#,##0.00"
                    },
                    {
                        "name": f"{col}_Max",
                        "expression": f"MAX(Table1[{col}])",
                        "formatString": "#,##0.00"
                    }
                ])
            elif dtype == 'datetime64[ns]':
                col_type = "DateTime"
            else:
                col_type = "String"
            
            columns.append({
                "name": col,
                "dataType": col_type,
                "sourceColumn": col,
                "formatString": "",
                "summarizeBy": "none" if col_type == "String" else "sum"
            })
        
        # Ajouter une mesure pour compter les lignes
        measures.append({
            "name": "Nombre_Total",
            "expression": "COUNTROWS(Table1)",
            "formatString": "#,##0"
        })
        
        return {
            "name": "DataModel",
            "compatibilityLevel": 1550,
            "model": {
                "culture": "fr-FR",
                "dataAccessOptions": {
                    "legacyRedirects": True,
                    "returnErrorValuesAsNull": True
                },
                "tables": [{
                    "name": "Table1",
                    "columns": columns,
                    "measures": measures,
                    "partitions": [{
                        "name": "Partition1",
                        "mode": "import",
                        "source": {
                            "type": "m",
                            "expression": f"let\n    Source = Excel.Workbook(File.Contents(\"{filename}\"), null, true)\nin\n    Source"
                        }
                    }]
                }],
                "relationships": [],
                "annotations": [{
                    "name": "ClientCompatibilityLevel",
                    "value": "600"
                }]
            }
        }
    
    def _create_metadata(self) -> dict:
        """
        Cr√©e les m√©tadonn√©es du template
        
        CONTENU:
        - Version du template
        - Date de cr√©ation
        - Informations syst√®me
        """
        return {
            "version": "4.0",
            "created": datetime.now().isoformat(),
            "lastModified": datetime.now().isoformat(),
            "creator": "Data Analytics Dashboard"
        }
    
    def _create_connections(self, filename: str) -> dict:
        """
        Configure la source de donn√©es
        
        IMPORTANT:
        - D√©finit comment Power BI se connecte aux donn√©es
        - Type: fichier, base de donn√©es, web, etc.
        - L'utilisateur devra mettre √† jour le chemin apr√®s import
        """
        return {
            "Version": 1,
            "Connections": [{
                "Name": "DataSource1",
                "ConnectionString": f"Provider=Microsoft.ACE.OLEDB.12.0;Data Source={filename};Extended Properties=\"Excel 12.0 Xml;HDR=YES\"",
                "ConnectionType": "OleDb"
            }],
            "RemoteArtifacts": []
        }
    
    def create_dax_measures_file(self, df: pd.DataFrame) -> str:
        """
        BONUS: G√©n√®re un fichier .dax avec toutes les mesures
        
        UTILIT√â:
        - Fichier texte avec mesures DAX pr√™tes √† copier-coller
        - Mesures avanc√©es (YTD, ratios, pourcentages, etc.)
        - Facilite l'enrichissement du mod√®le
        """
        measures = []
        
        measures.append("// ============================================")
        measures.append("// MESURES DE BASE")
        measures.append("// ============================================\n")
        
        # Mesures pour colonnes num√©riques
        for col in df.select_dtypes(include=['number']).columns:
            measures.append(f"// Mesures pour: {col}")
            measures.append(f"{col}_Total = SUM(Table1[{col}])")
            measures.append(f"{col}_Moyenne = AVERAGE(Table1[{col}])")
            measures.append(f"{col}_M√©diane = MEDIAN(Table1[{col}])")
            measures.append(f"{col}_Min = MIN(Table1[{col}])")
            measures.append(f"{col}_Max = MAX(Table1[{col}])")
            measures.append(f"{col}_EcartType = STDEV.P(Table1[{col}])")
            measures.append("")
        
        measures.append("\n// ============================================")
        measures.append("// MESURES DE COMPTAGE")
        measures.append("// ============================================\n")
        
        measures.append("Nombre_Total_Lignes = COUNTROWS(Table1)")
        measures.append("Nombre_Lignes_Distinctes = DISTINCTCOUNT(Table1[" + df.columns[0] + "])")
        
        measures.append("\n// ============================================")
        measures.append("// MESURES CONDITIONNELLES (Exemples)")
        measures.append("// ============================================\n")
        
        if len(df.select_dtypes(include=['number']).columns) > 0:
            num_col = df.select_dtypes(include=['number']).columns[0]
            measures.append(f"// Compte si {num_col} > moyenne")
            measures.append(f"Compte_Superieur_Moyenne = ")
            measures.append(f"CALCULATE(")
            measures.append(f"    COUNTROWS(Table1),")
            measures.append(f"    Table1[{num_col}] > [{num_col}_Moyenne]")
            measures.append(f")")
        
        measures.append("\n// ============================================")
        measures.append("// MESURES TEMPORELLES (si date pr√©sente)")
        measures.append("// ============================================\n")
        
        datetime_cols = df.select_dtypes(include=['datetime']).columns
        if len(datetime_cols) > 0:
            date_col = datetime_cols[0]
            if len(df.select_dtypes(include=['number']).columns) > 0:
                val_col = df.select_dtypes(include=['number']).columns[0]
                measures.append(f"// Calculs Year-To-Date pour {val_col}")
                measures.append(f"{val_col}_YTD = TOTALYTD([{val_col}_Total], Table1[{date_col}])")
                measures.append(f"{val_col}_MTD = TOTALMTD([{val_col}_Total], Table1[{date_col}])")
        
        return "\n".join(measures)
    
    def create_excel_with_data(self, df: pd.DataFrame, filename: str) -> bytes:
        """
        Cr√©e un fichier Excel propre pour accompagner le template
        
        POURQUOI:
        - Le .pbit contient la structure mais pas les donn√©es
        - L'Excel accompagne le template avec les donn√©es r√©elles
        - L'utilisateur importe l'Excel dans le template
        """
        output = io.BytesIO()
        
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Nettoyer les noms de colonnes pour Power BI
            df_clean = df.copy()
            df_clean.columns = [str(col).strip().replace(' ', '_').replace('[', '').replace(']', '') 
                               for col in df_clean.columns]
            
            # √âcrire les donn√©es
            df_clean.to_excel(writer, sheet_name='Data', index=False)
            
            # Ajouter une feuille avec instructions
            instructions = pd.DataFrame({
                '√âtape': [1, 2, 3, 4, 5],
                'Action': [
                    'Ouvrir Power BI Desktop',
                    'Ouvrir le fichier .pbit t√©l√©charg√©',
                    'Cliquer sur "Obtenir les donn√©es" > "Excel"',
                    f'S√©lectionner ce fichier ({filename})',
                    'S√©lectionner la feuille "Data" et cliquer sur "Charger"'
                ]
            })
            instructions.to_excel(writer, sheet_name='Instructions', index=False)
        
        output.seek(0)
        return output.getvalue()



class UIComponents:
    """Composants UI r√©utilisables pour le dashboard"""

    def __init__(self):
        pass

    def apply_styles(self):
        """Appliquer les styles CSS globaux"""
        st.markdown("""
            <style>
                .main-header {
                    font-size: 2.5rem;
                    color: #1E88E5;
                    text-align: center;
                    margin-bottom: 2rem;
                }
                .section-header {
                    font-size: 1.5rem;
                    color: #1E88E5;
                    margin-top: 1.5rem;
                    margin-bottom: 1rem;
                }
                .stButton>button {
                    height: 3em;
                    width: 100%;
                    font-size: 1em;
                }
                .css-1cpxqw2 edgvbvh3 {
                    margin-top: -20px;
                }
            </style>
        """, unsafe_allow_html=True)

    def render_sidebar(self):
        """Afficher des infos ou logos dans la sidebar si besoin"""
        st.sidebar.markdown("## Param√®tres")
        st.sidebar.markdown("Ce dashboard vous permet :")
        st.sidebar.markdown("- d‚Äôimporter des fichiers de donn√©es")
        st.sidebar.markdown("- d‚Äôanalyser rapidement les colonnes")
        st.sidebar.markdown("- d‚Äôexporter vers PowerBI")
        st.sidebar.markdown("---")
        #st.sidebar.info("D√©velopp√© avec par [Votre Nom]")
        
        name = st.sidebar.text_input("Votre nom", value=st.session_state.get("user_name", ""))
        if name:
            st.session_state["user_name"] = name
        st.sidebar.info(f"D√©velopp√© par {st.session_state.get('user_name', '...')}")

    
    def display_file_info(self, uploaded_file):
        """Affiche les m√©tadonn√©es du fichier import√©"""
        st.markdown("#### Informations sur le fichier")
        file_details = {
            "Nom du fichier": uploaded_file.name,
            "Type MIME": uploaded_file.type,
            "Taille (KB)": f"{len(uploaded_file.getbuffer()) / 1024:.1f}"
        }
        st.json(file_details)




class DatabaseManager:
    """G√®re la base de donn√©es SQLite pour stocker les importations"""

    def __init__(self, db_path="data_imports.db"):
        self.db_path = db_path
        self.conn = None

    def init_db(self):
        """Initialise la base de donn√©es et la table si elle n'existe pas"""
        self.conn = sqlite3.connect(self.db_path)
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS uploads (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT,
                upload_date TEXT,
                file_type TEXT,
                rows INTEGER,
                columns INTEGER
            )
        """)
        self.conn.commit()

    def save_upload(self, filename: str, file_type: str, df: pd.DataFrame):
        """Sauvegarde les informations d'un fichier import√©"""
        if self.conn is None:
            self.init_db()

        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO uploads (filename, upload_date, file_type, rows, columns)
            VALUES (?, ?, ?, ?, ?)
        """, (
            filename,
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            file_type,
            df.shape[0],
            df.shape[1]
        ))
        self.conn.commit()

    def get_uploads(self) -> pd.DataFrame:
        """R√©cup√®re l'historique des fichiers import√©s"""
        if self.conn is None:
            self.init_db()

        try:
            df = pd.read_sql_query("SELECT * FROM uploads ORDER BY upload_date DESC", self.conn)
            return df
        except Exception as e:
            st.error(f"Erreur lors de la lecture de la base de donn√©es : {e}")
            return pd.DataFrame()

import streamlit as st
import pandas as pd
from pathlib import Path
import sys

# Ajouter le dossier utils au path
sys.path.append(str(Path(__file__).parent / "utils"))

#from data_extractor import DataExtractor
#from data_analyzer import DataAnalyzer
#from visualizer import DataVisualizer
#from powerbi_exporter import PowerBIExporter
#from database_manager import DatabaseManager
#from ui_components import UIComponents

# Configuration de la page
st.set_page_config(
    page_title="Data Analytics Dashboard",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialiser les composants
db_manager = DatabaseManager()
ui_components = UIComponents()
data_extractor = DataExtractor()
data_analyzer = DataAnalyzer()
data_visualizer = DataVisualizer()
powerbi_exporter = PowerBIExporter()

# Initialiser la base de donn√©es
db_manager.init_db()

# Appliquer les styles CSS
ui_components.apply_styles()

def main():
    st.markdown('<h1 class="main-header">üìä Dashboard Analytics Pro</h1>', unsafe_allow_html=True)
    
    # Barre lat√©rale
    ui_components.render_sidebar()
    #page = st.sidebar.radio("Navigation", ["üîÑ Importer", "üìà Analyser", "üìö Historique", "‚öôÔ∏è PowerBI"], label_visibility="collapsed")
    if 'page' not in st.session_state:
        st.session_state['page'] = "üîÑ Importer"

    page = st.sidebar.radio(
        "Navigation",
        ["üîÑ Importer", "üìà Analyser", "üìö Historique", "‚öôÔ∏è PowerBI"],
        index=["üîÑ Importer", "üìà Analyser", "üìö Historique", "‚öôÔ∏è PowerBI"].index(st.session_state['page'])
    )
    st.session_state['page'] = page


    # Navigation entre les pages
    if page == "üîÑ Importer":
        render_import_page()
    elif page == "üìà Analyser":
        render_analysis_page()
    elif page == "üìö Historique":
        render_history_page()
    else:
        render_powerbi_page()

def render_import_page():
    st.markdown('<h2 class="section-header">üîÑ Importer vos donn√©es</h2>', unsafe_allow_html=True)
    
    # Zone de drag & drop am√©lior√©e
    uploaded_file = st.file_uploader(
        "Glissez-d√©posez vos fichiers ici ou cliquez pour parcourir", 
        type=['csv', 'xlsx', 'xls', 'json', 'txt'],
        help="Formats support√©s: CSV, Excel (.xlsx, .xls), JSON, TXT"
    )
    
    if uploaded_file is not None:
        # Afficher les d√©tails du fichier
        ui_components.display_file_info(uploaded_file)
        
        # Extraction des donn√©es
        with st.spinner('üîÑ Extraction des donn√©es en cours...'):
            df = data_extractor.extract_data(uploaded_file)
        
        if df is not None and not df.empty:
            st.success("‚úÖ Donn√©es import√©es avec succ√®s!")
            
            # Aper√ßu des donn√©es
            st.markdown('<h3 class="section-header">üëÄ Aper√ßu des donn√©es</h3>', unsafe_allow_html=True)
            col1, col2, col3 = st.columns(3)

            with col1:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True, key="analyze_btn_1"):
                    st.switch_page("pages/analyze.py") if hasattr(st, 'switch_page') else st.rerun()

            with col2:
                csv_data = df.to_csv(index=False)
                st.download_button(
                    "üíæ T√©l√©charger CSV",
                    csv_data,
                    file_name=f"cleaned_{uploaded_file.name}.csv",
                    mime="text/csv",
                    use_container_width=True,
                    key="download_csv_main"
                )


            with col3:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True, key="analyze_btn_2"):
                    st.session_state['page'] = "üìà Analyser"
                    st.rerun()

            
            # Pr√©visualisation avec pagination
            st.dataframe(
                df.head(20),
                use_container_width=True,
                height=400
            )
            
            # Informations sur les colonnes
            if st.expander("üîç Informations d√©taill√©es sur les colonnes"):
                col_info = data_analyzer.get_column_info(df)
                st.dataframe(col_info, use_container_width=True)
            
            # Sauvegarder dans la session et la base
            st.session_state['data'] = df
            st.session_state['filename'] = uploaded_file.name
            
            # Sauvegarder dans la base de donn√©es
            db_manager.save_upload(uploaded_file.name, uploaded_file.type, df)
            
            # Actions disponibles
            st.markdown('<h3 class="section-header">üéØ Actions disponibles</h3>', unsafe_allow_html=True)
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True , key="analyze_now_btn_1"):
                    st.switch_page("pages/analyze.py") if hasattr(st, 'switch_page') else st.rerun()
            
            with col2:
                csv_data = df.to_csv(index=False)
                st.download_button(
                    "üíæ T√©l√©charger CSV",
                    csv_data,
                    file_name=f"cleaned_{uploaded_file.name}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            
            with col3:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True , key="analyze_now_btn_2"):
                    st.session_state['page'] = "üìà Analyser"
                    st.rerun()


def render_analysis_page():
    st.markdown('<h2 class="section-header">üìà Analyse des donn√©es</h2>', unsafe_allow_html=True)
    
    if 'data' not in st.session_state:
        st.warning("‚ö†Ô∏è Aucune donn√©e √† analyser. Veuillez d'abord importer un fichier.")
        if st.button("‚û°Ô∏è Aller √† l'importation", type="primary"):
            st.rerun()
        return
    
    df = st.session_state['data']
    filename = st.session_state.get('filename', 'donn√©es')
    
    # Onglets d'analyse
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìã Aper√ßu", "üìä Statistiques", "üìà Visualisations", 
        "üîç Qualit√©", "üéØ Insights"
    ])
    
    with tab1:
        st.markdown(f"### üìÅ Analyse de: **{filename}**")
        
        # M√©triques g√©n√©rales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("üìä Lignes", df.shape[0])
        with col2:
            st.metric("üìã Colonnes", df.shape[1])
        with col3:
            numeric_cols = len(df.select_dtypes(include=['number']).columns)
            st.metric("üî¢ Colonnes num√©riques", numeric_cols)
        with col4:
            cat_cols = len(df.select_dtypes(include=['object', 'category']).columns)
            st.metric("üìù Colonnes texte", cat_cols)
        
        # Aper√ßu des donn√©es avec options de filtrage
        st.markdown("#### üëÄ Aper√ßu des donn√©es")
        
        # Options de filtrage
        if st.checkbox("üîç Activer le filtrage"):
            selected_cols = st.multiselect(
                "S√©lectionner les colonnes √† afficher",
                df.columns.tolist(),
                default=df.columns.tolist()[:10]
            )
            if selected_cols:
                st.dataframe(df[selected_cols], use_container_width=True, height=400)
            else:
                st.dataframe(df, use_container_width=True, height=400)
        else:
            st.dataframe(df, use_container_width=True, height=400)
    
    with tab2:
        st.markdown("### üìä Statistiques descriptives")
        
        # Statistiques pour colonnes num√©riques
        numeric_df = df.select_dtypes(include=['number'])
        if not numeric_df.empty:
            st.markdown("#### üî¢ Colonnes num√©riques")
            st.dataframe(numeric_df.describe(), use_container_width=True)
            
            # Matrice de corr√©lation
            if len(numeric_df.columns) > 1:
                st.markdown("#### üîó Matrice de corr√©lation")
                corr_fig = data_visualizer.create_correlation_matrix(numeric_df)
                st.plotly_chart(corr_fig, use_container_width=True)
        
        # Statistiques pour colonnes cat√©gorielles
        cat_df = df.select_dtypes(include=['object', 'category'])
        if not cat_df.empty:
            st.markdown("#### üìù Colonnes cat√©gorielles")
            cat_stats = data_analyzer.get_categorical_stats(cat_df)
            st.dataframe(cat_stats, use_container_width=True)
    
    with tab3:
        st.markdown("### üìà Visualisations automatiques")
        
        # G√©n√©rer les visualisations
        charts = data_visualizer.auto_generate_charts(df)
        
        if not charts:
            st.info("‚ÑπÔ∏è Aucune visualisation automatique disponible pour ce jeu de donn√©es.")
        else:
            # Organisation en colonnes pour un meilleur affichage
            for i, (chart_type, name, fig) in enumerate(charts):
                if i % 2 == 0:
                    col1, col2 = st.columns(2)
                
                with col1 if i % 2 == 0 else col2:
                    #st.plotly_chart(fig, use_container_width=True)
                    st.plotly_chart(fig, use_container_width=True, key=f"{chart_type}_{i}")

    
    with tab4:
        st.markdown("### üîç Qualit√© des donn√©es")
        
        # Analyse de la qualit√©
        quality_report = data_analyzer.analyze_data_quality(df)
        
        # M√©triques de qualit√©
        col1, col2, col3 = st.columns(3)
        with col1:
            missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
            st.metric("‚ùå Valeurs manquantes", f"{missing_pct:.1f}%")
        
        with col2:
            duplicates = df.duplicated().sum()
            st.metric("üîÑ Lignes dupliqu√©es", duplicates)
        
        with col3:
            data_types = len(df.dtypes.unique())
            st.metric("üè∑Ô∏è Types de donn√©es", data_types)
        
        # D√©tail par colonne
        st.markdown("#### üìã D√©tail par colonne")
        st.dataframe(quality_report, use_container_width=True)
        
        # Visualisation des valeurs manquantes
        if df.isnull().any().any():
            missing_fig = data_visualizer.create_missing_data_heatmap(df)
            st.plotly_chart(missing_fig, use_container_width=True)
    
    with tab5:
        st.markdown("### üéØ Insights automatiques")
        
        # G√©n√©rer des insights
        insights = data_analyzer.generate_insights(df)
        
        for insight in insights:
            st.info(f"üí° {insight}")
        
        # Recommandations
        st.markdown("#### üéØ Recommandations")
        recommendations = data_analyzer.get_recommendations(df)
        
        for rec in recommendations:
            st.success(f"‚úÖ {rec}")

def render_history_page():
    st.markdown('<h2 class="section-header">üìö Historique des importations</h2>', unsafe_allow_html=True)
    
    uploads_df = db_manager.get_uploads()
    
    if uploads_df.empty:
        st.info("üì≠ Aucun historique d'importation disponible.")
    else:
        # Affichage avec colonnes personnalis√©es
        st.dataframe(
            uploads_df[['id', 'filename', 'upload_date', 'file_type', 'rows', 'columns']],
            use_container_width=True,
            column_config={
                "id": "ID",
                "filename": "Nom du fichier",
                "upload_date": "Date d'import",
                "file_type": "Type",
                "rows": "Lignes",
                "columns": "Colonnes"
            }
        )
        
        # S√©lection et rechargement
        if len(uploads_df) > 0:
            selected_id = st.selectbox(
                "üîÑ S√©lectionner une importation √† recharger",
                uploads_df['id'].tolist(),
                format_func=lambda x: f"ID {x}: {uploads_df[uploads_df['id']==x]['filename'].iloc[0]}"
            )
            
            if st.button("üîÑ Recharger cette importation", type="primary"):
                # Recharger les donn√©es (simulation)
                selected_row = uploads_df[uploads_df['id'] == selected_id].iloc[0]
                st.session_state['filename'] = selected_row['filename']
                st.success(f"‚úÖ Importation {selected_row['filename']} recharg√©e!")

def render_powerbi_page():
    st.markdown('<h2 class="section-header">‚öôÔ∏è Export PowerBI</h2>', unsafe_allow_html=True)
    
    if 'data' not in st.session_state:
        st.warning("‚ö†Ô∏è Aucune donn√©e √† exporter. Veuillez d'abord importer un fichier.")
        return
    
    df = st.session_state['data']
    filename = st.session_state.get('filename', 'data')
    
    st.markdown("### üéØ Pr√©paration pour PowerBI")
    
    # Options d'export
    col1, col2 = st.columns(2)
    
    with col1:
        export_format = st.selectbox(
            "Format d'export",
            ["Excel (.xlsx)", "CSV", "JSON", "Template PowerBI"]
        )
    
    with col2:
        include_metadata = st.checkbox("Inclure les m√©tadonn√©es", value=True)
    
    # Pr√©visualisation
    st.markdown("#### üëÄ Pr√©visualisation des donn√©es √† exporter")
    st.dataframe(df.head(), use_container_width=True)
    
    # G√©n√©ration des fichiers d'export
    if st.button("üöÄ G√©n√©rer l'export PowerBI", type="primary"):
        with st.spinner("‚è≥ G√©n√©ration en cours..."):
            exports = powerbi_exporter.create_powerbi_export(df, filename, include_metadata)
        
        st.success("‚úÖ Export g√©n√©r√© avec succ√®s!")
        
        # Boutons de t√©l√©chargement
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.download_button(
                "üìä T√©l√©charger Excel",
                exports['excel'],
                file_name=f"{filename}_powerbi.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="download_excel_pbi"
            )
        
        with col2:
            st.download_button(
                "üìã T√©l√©charger CSV",
                exports['csv'],
                file_name=f"{filename}_powerbi.csv",
                mime="text/csv",
                key="download_csv_pbi"
            )
        
        with col3:
            st.download_button(
                "‚öôÔ∏è Template PowerBI",
                exports['template'],
                file_name=f"{filename}_template.json",
                mime="application/json",
                key="download_template_pbi"
            )
        
        # Instructions PowerBI
        with st.expander("üìñ Instructions d'importation PowerBI"):
            st.markdown("""
            **Pour importer dans PowerBI Desktop:**
            
            1. **Via Excel:**
               - T√©l√©chargez le fichier Excel
               - Dans PowerBI: Accueil ‚Üí Obtenir les donn√©es ‚Üí Excel
               - S√©lectionnez le fichier t√©l√©charg√©
            
            2. **Via CSV:**
               - T√©l√©chargez le fichier CSV
               - Dans PowerBI: Accueil ‚Üí Obtenir les donn√©es ‚Üí Texte/CSV
               - S√©lectionnez le fichier CSV
            
            3. **Via Template:**
               - T√©l√©chargez le template JSON
               - Utilisez-le comme r√©f√©rence pour configurer vos visualisations
            """)

if __name__ == "__main__":
    main()