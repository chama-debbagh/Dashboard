
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import json
import io
import sqlite3
from datetime import datetime
from typing import Optional, List, Tuple, Any, Dict
from pathlib import Path
from io import StringIO
import sys
import chardet
import openpyxl
from plotly.subplots import make_subplots  # d√©j√† utilis√© dans create_distribution_comparison
import zipfile



class DataExtractor:
    """Classe pour extraire les donn√©es de diff√©rents formats de fichiers"""
    
    def __init__(self):
        self.supported_formats = ['csv', 'xlsx', 'xls', 'json', 'txt']
    
    def extract_data(self, uploaded_file) -> Optional[pd.DataFrame]:
        """
        Extrait les donn√©es du fichier upload√©
        
        Args:
            uploaded_file: Fichier upload√© via Streamlit
            
        Returns:
            pd.DataFrame ou None si erreur
        """
        try:
            file_extension = self._get_file_extension(uploaded_file.name)
            
            if file_extension not in self.supported_formats:
                st.error(f"Format de fichier non support√©: {file_extension}")
                return None
            
            # Dispatcher vers la m√©thode appropri√©e
            if file_extension in ['xlsx', 'xls']:
                return self._extract_from_excel(uploaded_file)
            elif file_extension == 'csv':
                return self._extract_from_csv(uploaded_file)
            elif file_extension == 'json':
                return self._extract_from_json(uploaded_file)
            elif file_extension == 'txt':
                return self._extract_from_txt(uploaded_file)
            
        except Exception as e:
            st.error(f"Erreur lors de l'extraction: {str(e)}")
            return None
    
    def _get_file_extension(self, filename: str) -> str:
        """Obtient l'extension du fichier"""
        return Path(filename).suffix.lower().lstrip('.')
    
    def _extract_from_excel(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier Excel"""
        try:
            # Lire le fichier Excel avec gestion des erreurs
            excel_file = pd.ExcelFile(uploaded_file)
            
            # Si plusieurs feuilles, demander √† l'utilisateur de choisir
            if len(excel_file.sheet_names) > 1:
                st.info(f"Le fichier contient {len(excel_file.sheet_names)} feuilles")
                selected_sheet = st.selectbox(
                    "S√©lectionnez la feuille √† importer:",
                    excel_file.sheet_names,
                    key="excel_sheet_selector"
                )
                df = pd.read_excel(uploaded_file, sheet_name=selected_sheet)
            else:
                df = pd.read_excel(uploaded_file, sheet_name=0)
            
            # Nettoyage initial
            df = self._clean_dataframe(df)
            
            st.success(f"Fichier Excel import√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
            return df
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation Excel: {str(e)}")
            return None
    
    def _extract_from_csv(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier CSV avec d√©tection automatique"""
        try:
            # Lire les premiers octets pour d√©tecter l'encodage
            raw_data = uploaded_file.read()
            uploaded_file.seek(0)  # Remettre le curseur au d√©but
            
            # D√©tecter l'encodage
            encoding_result = chardet.detect(raw_data)
            encoding = encoding_result['encoding'] if encoding_result['confidence'] > 0.7 else 'utf-8'
            
            st.info(f"üîç Encodage d√©tect√©: {encoding} (confiance: {encoding_result['confidence']:.2f})")
            
            # Essayer diff√©rents s√©parateurs et configurations
            separators = [',', ';', '\t', '|']
            best_df = None
            best_cols = 0
            
            for sep in separators:
                try:
                    uploaded_file.seek(0)
                    df_test = pd.read_csv(
                        uploaded_file, 
                        sep=sep, 
                        encoding=encoding,
                        low_memory=False,
                        na_values=['', 'NA', 'N/A', 'NULL', 'null', '#N/A']
                    )
                    
                    # Garder le DataFrame avec le plus de colonnes coh√©rentes
                    if len(df_test.columns) > best_cols and len(df_test.columns) > 1:
                        best_df = df_test
                        best_cols = len(df_test.columns)
                        best_sep = sep
                        
                except:
                    continue
            
            if best_df is None:
                st.error("Impossible de d√©terminer le format CSV")
                return None
            
            st.success(f"CSV import√© avec s√©parateur '{best_sep}': {best_df.shape[0]} lignes, {best_df.shape[1]} colonnes")
            
            # Nettoyage
            best_df = self._clean_dataframe(best_df)
            return best_df
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation CSV: {str(e)}")
            return None
    
    def _extract_from_json(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier JSON"""
        try:
            json_data = json.load(uploaded_file)
            
            # Gestion de diff√©rentes structures JSON
            if isinstance(json_data, list):
                df = pd.json_normalize(json_data)
            elif isinstance(json_data, dict):
                # Essayer de trouver une liste dans le dictionnaire
                for key, value in json_data.items():
                    if isinstance(value, list) and len(value) > 0:
                        df = pd.json_normalize(value)
                        st.info(f"Donn√©es extraites de la cl√©: '{key}'")
                        break
                else:
                    # Si pas de liste trouv√©e, normaliser le dictionnaire
                    df = pd.json_normalize([json_data])
            else:
                st.error("Structure JSON non support√©e")
                return None
            
            st.success(f"JSON import√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
            return self._clean_dataframe(df)
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation JSON: {str(e)}")
            return None
    
    def _extract_from_txt(self, uploaded_file) -> Optional[pd.DataFrame]:
        """Extrait les donn√©es d'un fichier texte"""
        try:
            # Lire le contenu du fichier
            content = uploaded_file.read().decode('utf-8')
            lines = content.strip().split('\n')
            
            if not lines:
                st.error("Fichier texte vide")
                return None
            
            # Essayer de d√©tecter un format tabulaire
            first_line = lines[0]
            
            # D√©tecter le s√©parateur le plus probable
            separators = ['\t', ',', ';', '|', ' ']
            best_sep = None
            max_cols = 0
            
            for sep in separators:
                cols = len(first_line.split(sep))
                if cols > max_cols:
                    max_cols = cols
                    best_sep = sep
            
            if max_cols < 2:
                # Traiter comme texte simple
                df = pd.DataFrame({'Contenu': lines})
                st.info("Fichier trait√© comme texte simple")
            else:
                # Traiter comme donn√©es tabulaires
                data = []
                headers = lines[0].split(best_sep)
                
                for line in lines[1:]:
                    if line.strip():
                        values = line.split(best_sep)
                        # Ajuster la longueur si n√©cessaire
                        while len(values) < len(headers):
                            values.append('')
                        data.append(values[:len(headers)])
                
                df = pd.DataFrame(data, columns=headers)
                st.info(f"Fichier trait√© comme donn√©es tabulaires (s√©parateur: '{best_sep}')")
            
            st.success(f"Fichier texte import√©: {df.shape[0]} lignes, {df.shape[1]} colonnes")
            return self._clean_dataframe(df)
            
        except Exception as e:
            st.error(f"Erreur lors de l'importation du fichier texte: {str(e)}")
            return None
    
    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Nettoie le DataFrame apr√®s importation"""
        try:
            # Supprimer les lignes enti√®rement vides
            df = df.dropna(how='all')
            
            # Supprimer les colonnes enti√®rement vides
            df = df.dropna(axis=1, how='all')
            
            # Nettoyer les noms de colonnes
            df.columns = df.columns.astype(str)
            df.columns = [col.strip() for col in df.columns]
            
            # Remplacer les noms de colonnes vides
            df.columns = [f'Colonne_{i}' if col == '' or col.startswith('Unnamed') 
                         else col for i, col in enumerate(df.columns)]
            
            # Supprimer les doublons de noms de colonnes
            df.columns = pd.io.common.dedup_names(df.columns, is_potential_multiindex=False)
            
            # Tentative de conversion automatique des types
            df = self._auto_convert_types(df)
            
            return df
            
        except Exception as e:
            st.warning(f"Erreur lors du nettoyage: {str(e)}")
            return df
    
    def _auto_convert_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """Conversion automatique des types de donn√©es"""
        try:
            for col in df.columns:
                # Essayer de convertir en num√©rique
                numeric_col = pd.to_numeric(df[col], errors='coerce')
                if not numeric_col.isna().all():
                    # Si plus de 80% des valeurs sont num√©riques, convertir
                    valid_numeric = (~numeric_col.isna()).sum()
                    total_non_null = (~df[col].isna()).sum()
                    
                    if total_non_null > 0 and (valid_numeric / total_non_null) > 0.8:
                        df[col] = numeric_col
                        continue
                
                # Essayer de convertir en datetime
                try:
                    datetime_col = pd.to_datetime(df[col], errors='coerce')
                    valid_datetime = (~datetime_col.isna()).sum()
                    total_non_null = (~df[col].isna()).sum()
                    
                    if total_non_null > 0 and (valid_datetime / total_non_null) > 0.8:
                        df[col] = datetime_col
                        continue
                except:
                    pass
                
                # Nettoyer les colonnes texte
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str).str.strip()
                    df[col] = df[col].replace('nan', pd.NA)
            
            return df
            
        except Exception as e:
            st.warning(f"Erreur lors de la conversion des types: {str(e)}")
            return df






class DataAnalyzer:
    """Classe pour analyser les donn√©es et g√©n√©rer des insights"""
    
    def __init__(self):
        pass
    
    def get_column_info(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Retourne des informations d√©taill√©es sur chaque colonne
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            DataFrame avec les informations des colonnes
        """
        info_data = []
        
        for col in df.columns:
            col_data = {
                'Colonne': col,
                'Type': str(df[col].dtype),
                'Valeurs_uniques': df[col].nunique(),
                'Valeurs_manquantes': df[col].isnull().sum(),
                'Pourcentage_manquant': f"{(df[col].isnull().sum() / len(df)) * 100:.1f}%",
                'Taille_m√©moire_KB': f"{df[col].memory_usage(deep=True) / 1024:.1f}"
            }
            
            # Ajouter des statistiques sp√©cifiques selon le type
            if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                col_data.update({
                    'Min': df[col].min() if not df[col].empty else None,
                    'Max': df[col].max() if not df[col].empty else None,
                    'Moyenne': f"{df[col].mean():.2f}" if not df[col].empty else None,
                    'M√©diane': f"{df[col].median():.2f}" if not df[col].empty else None
                })
            elif df[col].dtype == 'object':
                col_data.update({
                    'Longueur_min': df[col].astype(str).str.len().min() if not df[col].empty else None,
                    'Longueur_max': df[col].astype(str).str.len().max() if not df[col].empty else None,
                    'Longueur_moyenne': f"{df[col].astype(str).str.len().mean():.1f}" if not df[col].empty else None,
                    'Valeur_fr√©quente': df[col].mode().iloc[0] if not df[col].mode().empty else None
                })
            
            info_data.append(col_data)
        
        return pd.DataFrame(info_data)
    
    def get_categorical_stats(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Statistiques pour les colonnes cat√©gorielles
        
        Args:
            df: DataFrame avec colonnes cat√©gorielles
            
        Returns:
            DataFrame avec statistiques cat√©gorielles
        """
        stats_data = []
        
        for col in df.columns:
            if df[col].nunique() < 50:  # Seulement pour colonnes avec peu de valeurs uniques
                value_counts = df[col].value_counts()
                stats_data.append({
                    'Colonne': col,
                    'Valeurs_uniques': df[col].nunique(),
                    'Valeur_dominante': value_counts.index[0] if len(value_counts) > 0 else None,
                    'Fr√©quence_dominante': value_counts.iloc[0] if len(value_counts) > 0 else 0,
                    'Pourcentage_dominante': f"{(value_counts.iloc[0] / len(df)) * 100:.1f}%" if len(value_counts) > 0 else "0%",
                    'Entropie': self._calculate_entropy(df[col])
                })
        
        return pd.DataFrame(stats_data)
    
    def analyze_data_quality(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Analyse la qualit√© des donn√©es
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            DataFrame avec rapport de qualit√©
        """
        quality_data = []
        
        for col in df.columns:
            # Calculs de base
            total_rows = len(df)
            missing_count = df[col].isnull().sum()
            missing_pct = (missing_count / total_rows) * 100
            unique_count = df[col].nunique()
            unique_pct = (unique_count / total_rows) * 100
            
            # Score de qualit√© (0-100)
            quality_score = 100
            if missing_pct > 50:
                quality_score -= 30
            elif missing_pct > 20:
                quality_score -= 15
            elif missing_pct > 5:
                quality_score -= 5
            
            # Probl√®mes potentiels
            issues = []
            if missing_pct > 20:
                issues.append("Beaucoup de valeurs manquantes")
            if unique_pct < 1 and df[col].dtype in ['object']:
                issues.append("Peu de diversit√©")
            if unique_count == 1:
                issues.append("Valeur constante")
            if df[col].dtype == 'object':
                # V√©rifier la coh√©rence des formats
                sample_values = df[col].dropna().astype(str).str.strip()
                if len(sample_values) > 0:
                    lengths = sample_values.str.len()
                    if lengths.std() > lengths.mean():
                        issues.append("Formats incoh√©rents")
            
            quality_data.append({
                'Colonne': col,
                'Score_qualit√©': f"{quality_score:.0f}/100",
                'Valeurs_manquantes': f"{missing_count} ({missing_pct:.1f}%)",
                'Valeurs_uniques': f"{unique_count} ({unique_pct:.1f}%)",
                'Probl√®mes': "; ".join(issues) if issues else "Aucun",
                'Recommandation': self._get_quality_recommendation(missing_pct, unique_pct, issues)
            })
        
        return pd.DataFrame(quality_data)
    
    def generate_insights(self, df: pd.DataFrame) -> List[str]:
        """
        G√©n√®re des insights automatiques sur les donn√©es
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            Liste d'insights
        """
        insights = []
        
        # Insights g√©n√©raux
        total_rows = len(df)
        total_cols = len(df.columns)
        
        insights.append(f"Le dataset contient {total_rows:,} lignes et {total_cols} colonnes")
        
        # Insights sur les valeurs manquantes
        missing_total = df.isnull().sum().sum()
        missing_pct = (missing_total / (total_rows * total_cols)) * 100
        if missing_pct > 10:
            insights.append(f"Attention: {missing_pct:.1f}% des donn√©es sont manquantes")
        elif missing_pct == 0:
            insights.append("Excellent: Aucune valeur manquante d√©tect√©e")
        
        # Insights sur les types de donn√©es
        numeric_cols = len(df.select_dtypes(include=['number']).columns)
        text_cols = len(df.select_dtypes(include=['object']).columns)
        date_cols = len(df.select_dtypes(include=['datetime']).columns)
        
        if numeric_cols > text_cols:
            insights.append(f"Dataset majoritairement num√©rique ({numeric_cols} colonnes num√©riques vs {text_cols} textuelles)")
        elif text_cols > numeric_cols:
            insights.append(f"Dataset majoritairement textuel ({text_cols} colonnes textuelles vs {numeric_cols} num√©riques)")
        
        if date_cols > 0:
            insights.append(f"Dataset temporel d√©tect√© avec {date_cols} colonne(s) de dates")
        
        # Insights sur la distribution
        for col in df.select_dtypes(include=['number']).columns[:3]:  # Top 3 colonnes num√©riques
            skewness = df[col].skew()
            if abs(skewness) > 2:
                skew_type = "tr√®s asym√©trique √† droite" if skewness > 0 else "tr√®s asym√©trique √† gauche"
                insights.append(f"La colonne '{col}' a une distribution {skew_type}")
        
        # Insights sur les corr√©lations
        numeric_df = df.select_dtypes(include=['number'])
        if len(numeric_df.columns) > 1:
            corr_matrix = numeric_df.corr()
            # Trouver les corr√©lations les plus fortes (hors diagonale)
            corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.7:
                        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))
            
            if corr_pairs:
                best_corr = max(corr_pairs, key=lambda x: abs(x[2]))
                insights.append(f"Forte corr√©lation d√©tect√©e entre '{best_corr[0]}' et '{best_corr[1]}' (r={best_corr[2]:.2f})")
        
        # Insights sur les outliers
        for col in df.select_dtypes(include=['number']).columns[:2]:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
            
            if len(outliers) > 0:
                outlier_pct = (len(outliers) / len(df)) * 100
                insights.append(f"La colonne '{col}' contient {len(outliers)} valeurs aberrantes ({outlier_pct:.1f}%)")
        
        return insights[:8]  # Limiter √† 8 insights
    
    def get_recommendations(self, df: pd.DataFrame) -> List[str]:
        """
        G√©n√®re des recommandations pour am√©liorer les donn√©es
        
        Args:
            df: DataFrame √† analyser
            
        Returns:
            Liste de recommandations
        """
        recommendations = []
        
        # Recommandations sur les valeurs manquantes
        high_missing_cols = []
        for col in df.columns:
            missing_pct = (df[col].isnull().sum() / len(df)) * 100
            if missing_pct > 20:
                high_missing_cols.append((col, missing_pct))
        
        if high_missing_cols:
            recommendations.append(f"Traiter les valeurs manquantes dans {len(high_missing_cols)} colonne(s): " + 
                                 ", ".join([f"{col} ({pct:.1f}%)" for col, pct in high_missing_cols[:3]]))
        
        # Recommandations sur les doublons
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            recommendations.append(f"Supprimer {duplicates} ligne(s) dupliqu√©e(s)")
        
        # Recommandations sur les types de donn√©es
        for col in df.select_dtypes(include=['object']).columns:
            # V√©rifier si la colonne pourrait √™tre num√©rique
            try:
                numeric_converted = pd.to_numeric(df[col], errors='coerce')
                non_null_original = df[col].notna().sum()
                non_null_converted = numeric_converted.notna().sum()
                
                if non_null_converted / non_null_original > 0.8:
                    recommendations.append(f"Convertir la colonne '{col}' en type num√©rique")
            except:
                pass
            
            # V√©rifier si la colonne pourrait √™tre cat√©gorielle
            if df[col].nunique() < 20 and df[col].nunique() / len(df) < 0.1:
                recommendations.append(f"Convertir la colonne '{col}' en type cat√©goriel pour optimiser la m√©moire")
        
        # Recommandations sur la normalisation
        numeric_cols = df.select_dtypes(include=['number']).columns
        for col in numeric_cols:
            if df[col].std() > 0:
                col_range = df[col].max() - df[col].min()
                col_mean = df[col].mean()
                if col_range > 1000 or col_mean > 1000:
                    recommendations.append(f"Consid√©rer la normalisation de la colonne '{col}' pour les analyses")
        
        # Recommandations sur l'indexation
        if len(df) > 10000:
            recommendations.append("Consid√©rer l'ajout d'un index pour am√©liorer les performances sur ce large dataset")
        
        # Recommandations sur les visualisations
        if len(numeric_cols) >= 2:
            recommendations.append("Cr√©er des graphiques de corr√©lation pour explorer les relations entre variables")
        
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            recommendations.append("Analyser la distribution des variables cat√©gorielles avec des graphiques en barres")
        
        return recommendations[:6]  # Limiter √† 6 recommandations
    
    def _calculate_entropy(self, series: pd.Series) -> float:
        """Calcule l'entropie d'une s√©rie (mesure de diversit√©)"""
        try:
            value_counts = series.value_counts()
            probabilities = value_counts / len(series)
            entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))
            return round(entropy, 3)
        except:
            return 0.0
    
    def _get_quality_recommendation(self, missing_pct: float, unique_pct: float, issues: List[str]) -> str:
        """G√©n√®re une recommandation bas√©e sur la qualit√© de la colonne"""
        if missing_pct > 50:
            return "Consid√©rer la suppression de cette colonne"
        elif missing_pct > 20:
            return "Imputer les valeurs manquantes"
        elif "Valeur constante" in issues:
            return "Supprimer cette colonne (pas d'information)"
        elif "Formats incoh√©rents" in issues:
            return "Standardiser le format des donn√©es"
        elif unique_pct < 1:
            return "V√©rifier la coh√©rence des donn√©es"
        else:
            return "Colonne de bonne qualit√©"




class DataVisualizer:
    """Classe pour cr√©er des visualisations automatiques des donn√©es"""
    
    def __init__(self):
        # Palette de couleurs moderne
        self.color_palette = [
            '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
            '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'
        ]
        
        # Template de style pour les graphiques
        self.layout_template = {
            'font': {'family': 'Arial, sans-serif', 'size': 12},
            'title': {'x': 0.5, 'xanchor': 'center'},
            'plot_bgcolor': 'white',
            'paper_bgcolor': 'white',
            'margin': {'l': 60, 'r': 60, 't': 80, 'b': 60}
        }
    
    def auto_generate_charts(self, df: pd.DataFrame) -> List[Tuple[str, str, Any]]:
        """
        G√©n√®re automatiquement des graphiques appropri√©s selon les donn√©es
        
        Args:
            df: DataFrame √† visualiser
            
        Returns:
            Liste de tuples (type_graphique, nom, figure_plotly)
        """
        charts = []
        
        # S√©parer les colonnes par type
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime']).columns.tolist()
        
        # 1. Histogrammes pour colonnes num√©riques
        for col in numeric_cols[:4]:  # Limiter √† 4 pour √©viter la surcharge
            fig = self._create_histogram(df, col)
            charts.append(("histogram", f"Distribution de {col}", fig))
        
        # 2. Graphiques en barres pour colonnes cat√©gorielles
        for col in categorical_cols[:3]:
            if df[col].nunique() <= 15:  # Seulement si peu de cat√©gories
                fig = self._create_bar_chart(df, col)
                charts.append(("bar", f"R√©partition de {col}", fig))
        
        # 3. Scatter plots pour paires de variables num√©riques
        if len(numeric_cols) >= 2:
            # Cr√©er scatter plot pour les 2 premi√®res colonnes num√©riques
            fig = self._create_scatter_plot(df, numeric_cols[0], numeric_cols[1])
            charts.append(("scatter", f"{numeric_cols[0]} vs {numeric_cols[1]}", fig))
            
            # Si plus de 2 colonnes num√©riques, ajouter un autre scatter plot
            if len(numeric_cols) >= 3:
                fig = self._create_scatter_plot(df, numeric_cols[0], numeric_cols[2])
                charts.append(("scatter", f"{numeric_cols[0]} vs {numeric_cols[2]}", fig))
        
        # 4. Box plots pour distribution par cat√©gorie
        if len(numeric_cols) >= 1 and len(categorical_cols) >= 1:
            # Prendre la premi√®re colonne cat√©gorielle avec peu de valeurs uniques
            cat_col = None
            for col in categorical_cols:
                if df[col].nunique() <= 10:
                    cat_col = col
                    break
            
            if cat_col:
                fig = self._create_box_plot(df, numeric_cols[0], cat_col)
                charts.append(("box", f"{numeric_cols[0]} par {cat_col}", fig))
        
        # 5. Graphiques temporels si colonnes datetime
        if len(datetime_cols) >= 1 and len(numeric_cols) >= 1:
            fig = self._create_time_series(df, datetime_cols[0], numeric_cols[0])
            charts.append(("timeseries", f"√âvolution de {numeric_cols[0]}", fig))
        
        # 6. Heatmap de corr√©lation si suffisamment de colonnes num√©riques
        if len(numeric_cols) >= 3:
            fig = self.create_correlation_matrix(df[numeric_cols])
            charts.append(("heatmap", "Matrice de corr√©lation", fig))
        
        return charts
    
    def _create_histogram(self, df: pd.DataFrame, column: str) -> go.Figure:
        """Cr√©e un histogramme pour une colonne num√©rique"""
        fig = px.histogram(
            df, 
            x=column,
            nbins=30,
            title=f"Distribution de {column}",
            color_discrete_sequence=[self.color_palette[0]]
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=column,
            yaxis_title="Fr√©quence",
            showlegend=False
        )
        
        # Ajouter ligne de moyenne
        mean_val = df[column].mean()
        fig.add_vline(
            x=mean_val, 
            line_dash="dash", 
            line_color="red",
            annotation_text=f"Moyenne: {mean_val:.2f}"
        )
        
        return fig
    
    def _create_bar_chart(self, df: pd.DataFrame, column: str) -> go.Figure:
        """Cr√©e un graphique en barres pour une colonne cat√©gorielle"""
        value_counts = df[column].value_counts().head(10)  # Top 10
        
        fig = px.bar(
            x=value_counts.index,
            y=value_counts.values,
            title=f"R√©partition de {column}",
            color=value_counts.values,
            color_continuous_scale="viridis"
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=column,
            yaxis_title="Fr√©quence",
            showlegend=False
        )
        
        # Rotation des labels si n√©cessaires
        if max([len(str(x)) for x in value_counts.index]) > 10:
            fig.update_xaxes(tickangle=45)
        
        return fig
    
    def _create_scatter_plot(self, df: pd.DataFrame, x_col: str, y_col: str) -> go.Figure:
        """Cr√©e un scatter plot entre deux colonnes num√©riques"""
        fig = px.scatter(
            df,
            x=x_col,
            y=y_col,
            title=f"Relation entre {x_col} et {y_col}",
            color_discrete_sequence=[self.color_palette[2]],
            opacity=0.7
        )
        
        # Ajouter ligne de tendance
        try:
            fig.add_traces(
                px.scatter(df, x=x_col, y=y_col, trendline="ols").data[1]
            )
        except:
            pass
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=x_col,
            yaxis_title=y_col
        )
        
        # Calculer et afficher la corr√©lation
        correlation = df[x_col].corr(df[y_col])
        fig.add_annotation(
            x=0.02, y=0.98,
            xref="paper", yref="paper",
            text=f"Corr√©lation: {correlation:.3f}",
            showarrow=False,
            bgcolor="white",
            bordercolor="black",
            borderwidth=1
        )
        
        return fig
    
    def _create_box_plot(self, df: pd.DataFrame, numeric_col: str, cat_col: str) -> go.Figure:
        """Cr√©e un box plot pour analyser la distribution d'une variable num√©rique par cat√©gorie"""
        fig = px.box(
            df,
            x=cat_col,
            y=numeric_col,
            title=f"Distribution de {numeric_col} par {cat_col}",
            color=cat_col
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title=cat_col,
            yaxis_title=numeric_col
        )
        
        if df[cat_col].nunique() > 5:
            fig.update_xaxes(tickangle=45)
        
        return fig
    
    def _create_time_series(self, df: pd.DataFrame, date_col: str, value_col: str) -> go.Figure:
        """Cr√©e un graphique temporel"""
        # Trier par date
        df_sorted = df.sort_values(date_col)
        
        fig = px.line(
            df_sorted,
            x=date_col,
            y=value_col,
            title=f"√âvolution temporelle de {value_col}",
            color_discrete_sequence=[self.color_palette[1]]
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title="Date",
            yaxis_title=value_col
        )
        
        return fig
    
    def create_correlation_matrix(self, df: pd.DataFrame) -> go.Figure:
        """Cr√©e une heatmap de corr√©lation"""
        # Calculer la matrice de corr√©lation
        corr_matrix = df.corr()
        
        # Cr√©er la heatmap
        fig = px.imshow(
            corr_matrix,
            text_auto=True,
            aspect="auto",
            title="Matrice de corr√©lation",
            color_continuous_scale="RdBu_r",
            zmin=-1,
            zmax=1
        )
        
        fig.update_layout(
            **self.layout_template,
            width=600,
            height=500
        )
        
        return fig
    
    def create_missing_data_heatmap(self, df: pd.DataFrame) -> go.Figure:
        """Cr√©e une heatmap des valeurs manquantes"""
        # Cr√©er matrice des valeurs manquantes
        missing_data = df.isnull().astype(int)
        
        fig = px.imshow(
            missing_data.T,  # Transposer pour avoir colonnes en y
            title="Carte des valeurs manquantes (blanc = manquant)",
            color_continuous_scale=["white", "red"],
            aspect="auto"
        )
        
        fig.update_layout(
            **self.layout_template,
            xaxis_title="Index des lignes",
            yaxis_title="Colonnes",
            height=400
        )
        
        return fig
    
    def create_distribution_comparison(self, df: pd.DataFrame, columns: List[str]) -> go.Figure:
        """Compare la distribution de plusieurs colonnes num√©riques"""
        fig = make_subplots(
            rows=1, cols=len(columns),
            subplot_titles=columns,
            shared_yaxes=True
        )
        
        for i, col in enumerate(columns):
            fig.add_trace(
                go.Histogram(
                    x=df[col],
                    name=col,
                    marker_color=self.color_palette[i % len(self.color_palette)],
                    opacity=0.7
                ),
                row=1, col=i+1
            )
        
        fig.update_layout(
            title="Comparaison des distributions",
            **self.layout_template,
            height=400,
            showlegend=False
        )
        
        return fig
    
    def create_statistical_summary_chart(self, df: pd.DataFrame) -> go.Figure:
        """Cr√©e un graphique r√©sum√© des statistiques"""
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        
        if len(numeric_cols) == 0:
            return None
        
        # Calculer les statistiques
        stats = df[numeric_cols].describe().T
        
        fig = go.Figure()
        
        # Ajouter les barres pour moyenne et m√©diane
        fig.add_trace(go.Bar(
            name='Moyenne',
            x=stats.index,
            y=stats['mean'],
            marker_color=self.color_palette[0]
        ))
        
        fig.add_trace(go.Bar(
            name='M√©diane',
            x=stats.index,
            y=stats['50%'],
            marker_color=self.color_palette[1]
        ))
        
        fig.update_layout(
            title="Comparaison Moyenne vs M√©diane",
            **self.layout_template,
            barmode='group',
            xaxis_title="Variables",
            yaxis_title="Valeurs"
        )
        
        return fig






class PowerBIExporter:
    """Classe pour exporter les donn√©es vers PowerBI"""
    
    def __init__(self):
        pass
    
    def create_powerbi_export(self, df: pd.DataFrame, filename: str, include_metadata: bool = True) -> Dict[str, Any]:
        """
        Cr√©e tous les fichiers n√©cessaires pour PowerBI
        
        Args:
            df: DataFrame √† exporter
            filename: Nom du fichier original
            include_metadata: Inclure les m√©tadonn√©es
            
        Returns:
            Dictionnaire avec les fichiers g√©n√©r√©s
        """
        exports = {}
        
        # 1. Export Excel optimis√© pour PowerBI
        exports['excel'] = self._create_excel_export(df, filename, include_metadata)
        
        # 2. Export CSV propre
        exports['csv'] = self._create_csv_export(df)
        
        # 3. Template PowerBI JSON
        exports['template'] = self._create_powerbi_template(df, filename, include_metadata)
        
        # 4. Fichier de mesures DAX
        exports['dax_measures'] = self._create_dax_measures(df)
        
        return exports
    
    def _create_excel_export(self, df: pd.DataFrame, filename: str, include_metadata: bool) -> bytes:
        """Cr√©e un fichier Excel optimis√© pour PowerBI"""
        output = io.BytesIO()
        
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Feuille principale avec les donn√©es
            df_clean = self._prepare_dataframe_for_powerbi(df)
            df_clean.to_excel(writer, sheet_name='Data', index=False)
            
            if include_metadata:
                # Feuille avec m√©tadonn√©es
                metadata = self._generate_metadata(df, filename)
                metadata_df = pd.DataFrame(list(metadata.items()), columns=['Propri√©t√©', 'Valeur'])
                metadata_df.to_excel(writer, sheet_name='Metadata', index=False)
                
                # Feuille avec informations sur les colonnes
                column_info = self._generate_column_info(df)
                column_info.to_excel(writer, sheet_name='Column_Info', index=False)
                
                # Feuille avec suggestions de visualisations
                viz_suggestions = self._generate_visualization_suggestions(df)
                viz_df = pd.DataFrame(viz_suggestions)
                viz_df.to_excel(writer, sheet_name='Viz_Suggestions', index=False)
        
        output.seek(0)
        return output.getvalue()
    
    def _create_csv_export(self, df: pd.DataFrame) -> str:
        """Cr√©e un export CSV propre pour PowerBI"""
        df_clean = self._prepare_dataframe_for_powerbi(df)
        return df_clean.to_csv(index=False, encoding='utf-8-sig')  # UTF-8 avec BOM pour PowerBI
    
    def _create_powerbi_template(self, df: pd.DataFrame, filename: str, include_metadata: bool) -> str:
        """Cr√©e un template JSON pour PowerBI"""
        template = {
            "version": "1.0",
            "name": f"Template for {filename}",
            "created_at": datetime.now().isoformat(),
            "columns": [{
                "name": col,
                "type": str(dtype)
            } for col, dtype in df.dtypes.items()]
        }
        
        if include_metadata:
            template["metadata"] = self._generate_metadata(df, filename)
        
        return json.dumps(template, indent=4, ensure_ascii=False)

    def _create_dax_measures(self, df: pd.DataFrame) -> str:
        """Cr√©e un script DAX avec des mesures courantes"""
        measures = []
        for col in df.select_dtypes(include='number').columns[:5]:  # Limiter √† 5 colonnes
            measures.append(f"{col}_Average = AVERAGE('{col}')")
            measures.append(f"{col}_Sum = SUM('{col}')")
            measures.append(f"{col}_Max = MAX('{col}')")
            measures.append(f"{col}_Min = MIN('{col}')")
        return "\n".join(measures)

    def _prepare_dataframe_for_powerbi(self, df: pd.DataFrame) -> pd.DataFrame:
        """Pr√©pare un DataFrame nettoy√© pour PowerBI"""
        df_clean = df.copy()
        df_clean.columns = [str(col).strip().replace(' ', '_') for col in df_clean.columns]
        return df_clean

    def _generate_metadata(self, df: pd.DataFrame, filename: str) -> Dict[str, str]:
        """G√©n√®re les m√©tadonn√©es g√©n√©rales"""
        return {
            "Nom du fichier": filename,
            "Nombre de lignes": str(len(df)),
            "Nombre de colonnes": str(len(df.columns)),
            "Date d'export": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    def _generate_column_info(self, df: pd.DataFrame) -> pd.DataFrame:
        """G√©n√®re un tableau d'information sur les colonnes"""
        data = []
        for col in df.columns:
            data.append({
                "Colonne": col,
                "Type": str(df[col].dtype),
                "Valeurs uniques": df[col].nunique(),
                "Valeurs manquantes": df[col].isnull().sum()
            })
        return pd.DataFrame(data)

    def _generate_visualization_suggestions(self, df: pd.DataFrame) -> list:
        """G√©n√®re des suggestions de visualisation"""
        suggestions = []
        for col in df.columns:
            dtype = df[col].dtype
            if dtype in ['int64', 'float64']:
                suggestions.append({
                    "Colonne": col,
                    "Suggestion": "Histogramme ou bo√Æte √† moustaches (boxplot)"
                })
            elif dtype == 'object' and df[col].nunique() < 30:
                suggestions.append({
                    "Colonne": col,
                    "Suggestion": "Diagramme en barres ou camembert"
                })
        return suggestions
    
    def create_pbids_for_csv(self, csv_filename: str, friendly_name: str = "Dataset") -> str:
        """
        G√©n√®re un contenu PBIDS (JSON) pointant vers un CSV local.
        Retourne une cha√Æne JSON (√† t√©l√©charger en .pbids).
        """
        pbids = {
            "version": "0.1",
            "connections": [
                {
                    "details": {
                        "protocol": "file",
                        "path": csv_filename  # ex: "./mon_export_powerbi.csv"
                    },
                    "name": friendly_name,
                    "mode": "import"
                }
            ]
        }
        return json.dumps(pbids, indent=4, ensure_ascii=False)

    def create_readme_for_powerbi(self, base_csv_name: str, base_filename: str) -> str:
        """
        G√©n√®re un README expliquant comment ouvrir le PBIDS et produire un .pbit dans Power BI Desktop.
        """
        return f"""# Template Power BI ‚Äì Guide rapide

    Ce dossier contient :
    - `{base_csv_name}` : donn√©es propres export√©es depuis le dashboard
    - `{base_filename}.pbids` : fichier **PBIDS** qui pr√©configure la source de donn√©es
    - `{base_filename}_measures.txt` : suggestions de mesures DAX
    - `{base_filename}_template.json` : m√©ta-infos (colonnes/types)

    ## Utilisation dans Power BI Desktop
    1. Placez **{base_csv_name}** et **{base_filename}.pbids** dans le **m√™me dossier**.
    2. **Double-cliquez** sur `{base_filename}.pbids` (ou `Fichier ‚Üí Ouvrir` dans Power BI Desktop).
    3. Validez la connexion et chargez les donn√©es.
    4. Cr√©ez vos visuels et, si vous voulez un mod√®le r√©utilisable :
    **Fichier ‚Üí Exporter ‚Üí Mod√®le Power BI (.pbit)**.

    Notes :
    - Le fichier **PBIDS** sert √† acc√©l√©rer *Get Data* ; le vrai mod√®le **.pbit** se sauvegarde depuis Power BI Desktop.
    - Un PBIDS r√©f√©rence **une seule source** par fichier (.pbids)."""

    def create_powerbi_template_package(self, df: pd.DataFrame, filename: str, include_metadata: bool = True) -> dict:
        """
        Cr√©e un pack complet: CSV, PBIDS, DAX, template JSON, et un ZIP pr√™t √† t√©l√©charger.
        Retourne un dict avec: csv_str, pbids_str, dax_str, template_json_str, zip_bytes
        """
        # 1) CSV propre (m√™me logique que _create_csv_export)
        df_clean = self._prepare_dataframe_for_powerbi(df)
        csv_str = df_clean.to_csv(index=False, encoding="utf-8-sig")

        # 2) PBIDS pointant vers ce CSV (nom attendu c√¥t√© utilisateur)
        csv_name = f"{filename}_powerbi.csv"
        pbids_str = self.create_pbids_for_csv(csv_name, friendly_name=filename)

        # 3) DAX (r√©utilise ta m√©thode existante)
        dax_str = self._create_dax_measures(df)

        # 4) Template JSON (r√©utilise ta m√©thode existante)
        template_json_str = self._create_powerbi_template(df, filename, include_metadata)

        # 5) README
        readme_str = self.create_readme_for_powerbi(csv_name, filename)

        # 6) ZIP tout-en-un
        zip_buf = io.BytesIO()
        with zipfile.ZipFile(zip_buf, "w", zipfile.ZIP_DEFLATED) as zf:
            zf.writestr(csv_name, csv_str)
            zf.writestr(f"{filename}.pbids", pbids_str)
            zf.writestr(f"{filename}_measures.txt", dax_str)
            zf.writestr(f"{filename}_template.json", template_json_str)
            zf.writestr("README.md", readme_str)
        zip_buf.seek(0)

        return {
            "csv_str": csv_str,
            "pbids_str": pbids_str,
            "dax_str": dax_str,
            "template_json_str": template_json_str,
            "zip_bytes": zip_buf.getvalue(),
            "csv_download_name": csv_name,
            "pbids_download_name": f"{filename}.pbids",
            "zip_download_name": f"{filename}_powerbi_template_pack.zip"
        }
        




class UIComponents:
    """Composants UI r√©utilisables pour le dashboard"""

    def __init__(self):
        pass

    def apply_styles(self):
        """Appliquer les styles CSS globaux"""
        st.markdown("""
            <style>
                .main-header {
                    font-size: 2.5rem;
                    color: #1E88E5;
                    text-align: center;
                    margin-bottom: 2rem;
                }
                .section-header {
                    font-size: 1.5rem;
                    color: #1E88E5;
                    margin-top: 1.5rem;
                    margin-bottom: 1rem;
                }
                .stButton>button {
                    height: 3em;
                    width: 100%;
                    font-size: 1em;
                }
                .css-1cpxqw2 edgvbvh3 {
                    margin-top: -20px;
                }
            </style>
        """, unsafe_allow_html=True)

    def render_sidebar(self):
        """Afficher des infos ou logos dans la sidebar si besoin"""
        st.sidebar.markdown("## Param√®tres")
        st.sidebar.markdown("Ce dashboard vous permet :")
        st.sidebar.markdown("- d‚Äôimporter des fichiers de donn√©es")
        st.sidebar.markdown("- d‚Äôanalyser rapidement les colonnes")
        st.sidebar.markdown("- d‚Äôexporter vers PowerBI")
        st.sidebar.markdown("---")
        #st.sidebar.info("D√©velopp√© avec par [Votre Nom]")
        
        name = st.sidebar.text_input("Votre nom", value=st.session_state.get("user_name", ""))
        if name:
            st.session_state["user_name"] = name
        st.sidebar.info(f"D√©velopp√© par {st.session_state.get('user_name', '...')}")

    
    def display_file_info(self, uploaded_file):
        """Affiche les m√©tadonn√©es du fichier import√©"""
        st.markdown("#### Informations sur le fichier")
        file_details = {
            "Nom du fichier": uploaded_file.name,
            "Type MIME": uploaded_file.type,
            "Taille (KB)": f"{len(uploaded_file.getbuffer()) / 1024:.1f}"
        }
        st.json(file_details)




class DatabaseManager:
    """G√®re la base de donn√©es SQLite pour stocker les importations"""

    def __init__(self, db_path="data_imports.db"):
        self.db_path = db_path
        self.conn = None

    def init_db(self):
        """Initialise la base de donn√©es et la table si elle n'existe pas"""
        self.conn = sqlite3.connect(self.db_path)
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS uploads (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT,
                upload_date TEXT,
                file_type TEXT,
                rows INTEGER,
                columns INTEGER
            )
        """)
        self.conn.commit()

    def save_upload(self, filename: str, file_type: str, df: pd.DataFrame):
        """Sauvegarde les informations d'un fichier import√©"""
        if self.conn is None:
            self.init_db()

        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO uploads (filename, upload_date, file_type, rows, columns)
            VALUES (?, ?, ?, ?, ?)
        """, (
            filename,
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            file_type,
            df.shape[0],
            df.shape[1]
        ))
        self.conn.commit()

    def get_uploads(self) -> pd.DataFrame:
        """R√©cup√®re l'historique des fichiers import√©s"""
        if self.conn is None:
            self.init_db()

        try:
            df = pd.read_sql_query("SELECT * FROM uploads ORDER BY upload_date DESC", self.conn)
            return df
        except Exception as e:
            st.error(f"Erreur lors de la lecture de la base de donn√©es : {e}")
            return pd.DataFrame()

import streamlit as st
import pandas as pd
from pathlib import Path
import sys

# Ajouter le dossier utils au path
sys.path.append(str(Path(__file__).parent / "utils"))

#from data_extractor import DataExtractor
#from data_analyzer import DataAnalyzer
#from visualizer import DataVisualizer
#from powerbi_exporter import PowerBIExporter
#from database_manager import DatabaseManager
#from ui_components import UIComponents

# Configuration de la page
st.set_page_config(
    page_title="Data Analytics Dashboard",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialiser les composants
db_manager = DatabaseManager()
ui_components = UIComponents()
data_extractor = DataExtractor()
data_analyzer = DataAnalyzer()
data_visualizer = DataVisualizer()
powerbi_exporter = PowerBIExporter()

# Initialiser la base de donn√©es
db_manager.init_db()

# Appliquer les styles CSS
ui_components.apply_styles()

def main():
    st.markdown('<h1 class="main-header">üìä Dashboard Analytics Pro</h1>', unsafe_allow_html=True)
    
    # Barre lat√©rale
    ui_components.render_sidebar()
    #page = st.sidebar.radio("Navigation", ["üîÑ Importer", "üìà Analyser", "üìö Historique", "‚öôÔ∏è PowerBI"], label_visibility="collapsed")
    if 'page' not in st.session_state:
        st.session_state['page'] = "üîÑ Importer"

    page = st.sidebar.radio(
        "Navigation",
        ["üîÑ Importer", "üìà Analyser", "üìö Historique", "‚öôÔ∏è PowerBI"],
        index=["üîÑ Importer", "üìà Analyser", "üìö Historique", "‚öôÔ∏è PowerBI"].index(st.session_state['page'])
    )
    st.session_state['page'] = page


    # Navigation entre les pages
    if page == "üîÑ Importer":
        render_import_page()
    elif page == "üìà Analyser":
        render_analysis_page()
    elif page == "üìö Historique":
        render_history_page()
    else:
        render_powerbi_page()

def render_import_page():
    st.markdown('<h2 class="section-header">üîÑ Importer vos donn√©es</h2>', unsafe_allow_html=True)
    
    # Zone de drag & drop am√©lior√©e
    uploaded_file = st.file_uploader(
        "Glissez-d√©posez vos fichiers ici ou cliquez pour parcourir", 
        type=['csv', 'xlsx', 'xls', 'json', 'txt'],
        help="Formats support√©s: CSV, Excel (.xlsx, .xls), JSON, TXT"
    )
    
    if uploaded_file is not None:
        # Afficher les d√©tails du fichier
        ui_components.display_file_info(uploaded_file)
        
        # Extraction des donn√©es
        with st.spinner('üîÑ Extraction des donn√©es en cours...'):
            df = data_extractor.extract_data(uploaded_file)
        
        if df is not None and not df.empty:
            st.success("‚úÖ Donn√©es import√©es avec succ√®s!")
            
            # Aper√ßu des donn√©es
            st.markdown('<h3 class="section-header">üëÄ Aper√ßu des donn√©es</h3>', unsafe_allow_html=True)
            col1, col2, col3 = st.columns(3)

            with col1:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True, key="analyze_btn_1"):
                    st.switch_page("pages/analyze.py") if hasattr(st, 'switch_page') else st.rerun()

            with col2:
                csv_data = df.to_csv(index=False)
                st.download_button(
                    "üíæ T√©l√©charger CSV",
                    csv_data,
                    file_name=f"cleaned_{uploaded_file.name}.csv",
                    mime="text/csv",
                    use_container_width=True,
                    key="download_csv_main"
                )


            with col3:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True, key="analyze_btn_2"):
                    st.session_state['page'] = "üìà Analyser"
                    st.rerun()

            
            # Pr√©visualisation avec pagination
            st.dataframe(
                df.head(20),
                use_container_width=True,
                height=400
            )
            
            # Informations sur les colonnes
            if st.expander("üîç Informations d√©taill√©es sur les colonnes"):
                col_info = data_analyzer.get_column_info(df)
                st.dataframe(col_info, use_container_width=True)
            
            # Sauvegarder dans la session et la base
            st.session_state['data'] = df
            st.session_state['filename'] = uploaded_file.name
            
            # Sauvegarder dans la base de donn√©es
            db_manager.save_upload(uploaded_file.name, uploaded_file.type, df)
            
            # Actions disponibles
            st.markdown('<h3 class="section-header">üéØ Actions disponibles</h3>', unsafe_allow_html=True)
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True , key="analyze_now_btn_1"):
                    st.switch_page("pages/analyze.py") if hasattr(st, 'switch_page') else st.rerun()
            
            with col2:
                csv_data = df.to_csv(index=False)
                st.download_button(
                    "üíæ T√©l√©charger CSV",
                    csv_data,
                    file_name=f"cleaned_{uploaded_file.name}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            
            with col3:
                if st.button("üìà Analyser maintenant", type="primary", use_container_width=True , key="analyze_now_btn_2"):
                    st.session_state['page'] = "üìà Analyser"
                    st.rerun()


def render_analysis_page():
    st.markdown('<h2 class="section-header">üìà Analyse des donn√©es</h2>', unsafe_allow_html=True)
    
    if 'data' not in st.session_state:
        st.warning("‚ö†Ô∏è Aucune donn√©e √† analyser. Veuillez d'abord importer un fichier.")
        if st.button("‚û°Ô∏è Aller √† l'importation", type="primary"):
            st.rerun()
        return
    
    df = st.session_state['data']
    filename = st.session_state.get('filename', 'donn√©es')
    
    # Onglets d'analyse
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìã Aper√ßu", "üìä Statistiques", "üìà Visualisations", 
        "üîç Qualit√©", "üéØ Insights"
    ])
    
    with tab1:
        st.markdown(f"### üìÅ Analyse de: **{filename}**")
        
        # M√©triques g√©n√©rales
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("üìä Lignes", df.shape[0])
        with col2:
            st.metric("üìã Colonnes", df.shape[1])
        with col3:
            numeric_cols = len(df.select_dtypes(include=['number']).columns)
            st.metric("üî¢ Colonnes num√©riques", numeric_cols)
        with col4:
            cat_cols = len(df.select_dtypes(include=['object', 'category']).columns)
            st.metric("üìù Colonnes texte", cat_cols)
        
        # Aper√ßu des donn√©es avec options de filtrage
        st.markdown("#### üëÄ Aper√ßu des donn√©es")
        
        # Options de filtrage
        if st.checkbox("üîç Activer le filtrage"):
            selected_cols = st.multiselect(
                "S√©lectionner les colonnes √† afficher",
                df.columns.tolist(),
                default=df.columns.tolist()[:10]
            )
            if selected_cols:
                st.dataframe(df[selected_cols], use_container_width=True, height=400)
            else:
                st.dataframe(df, use_container_width=True, height=400)
        else:
            st.dataframe(df, use_container_width=True, height=400)
    
    with tab2:
        st.markdown("### üìä Statistiques descriptives")
        
        # Statistiques pour colonnes num√©riques
        numeric_df = df.select_dtypes(include=['number'])
        if not numeric_df.empty:
            st.markdown("#### üî¢ Colonnes num√©riques")
            st.dataframe(numeric_df.describe(), use_container_width=True)
            
            # Matrice de corr√©lation
            if len(numeric_df.columns) > 1:
                st.markdown("#### üîó Matrice de corr√©lation")
                corr_fig = data_visualizer.create_correlation_matrix(numeric_df)
                st.plotly_chart(corr_fig, use_container_width=True)
        
        # Statistiques pour colonnes cat√©gorielles
        cat_df = df.select_dtypes(include=['object', 'category'])
        if not cat_df.empty:
            st.markdown("#### üìù Colonnes cat√©gorielles")
            cat_stats = data_analyzer.get_categorical_stats(cat_df)
            st.dataframe(cat_stats, use_container_width=True)
    
    with tab3:
        st.markdown("### üìà Visualisations automatiques")
        
        # G√©n√©rer les visualisations
        charts = data_visualizer.auto_generate_charts(df)
        
        if not charts:
            st.info("‚ÑπÔ∏è Aucune visualisation automatique disponible pour ce jeu de donn√©es.")
        else:
            # Organisation en colonnes pour un meilleur affichage
            for i, (chart_type, name, fig) in enumerate(charts):
                if i % 2 == 0:
                    col1, col2 = st.columns(2)
                
                with col1 if i % 2 == 0 else col2:
                    #st.plotly_chart(fig, use_container_width=True)
                    st.plotly_chart(fig, use_container_width=True, key=f"{chart_type}_{i}")

    
    with tab4:
        st.markdown("### üîç Qualit√© des donn√©es")
        
        # Analyse de la qualit√©
        quality_report = data_analyzer.analyze_data_quality(df)
        
        # M√©triques de qualit√©
        col1, col2, col3 = st.columns(3)
        with col1:
            missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
            st.metric("‚ùå Valeurs manquantes", f"{missing_pct:.1f}%")
        
        with col2:
            duplicates = df.duplicated().sum()
            st.metric("üîÑ Lignes dupliqu√©es", duplicates)
        
        with col3:
            data_types = len(df.dtypes.unique())
            st.metric("üè∑Ô∏è Types de donn√©es", data_types)
        
        # D√©tail par colonne
        st.markdown("#### üìã D√©tail par colonne")
        st.dataframe(quality_report, use_container_width=True)
        
        # Visualisation des valeurs manquantes
        if df.isnull().any().any():
            missing_fig = data_visualizer.create_missing_data_heatmap(df)
            st.plotly_chart(missing_fig, use_container_width=True)
    
    with tab5:
        st.markdown("### üéØ Insights automatiques")
        
        # G√©n√©rer des insights
        insights = data_analyzer.generate_insights(df)
        
        for insight in insights:
            st.info(f"üí° {insight}")
        
        # Recommandations
        st.markdown("#### üéØ Recommandations")
        recommendations = data_analyzer.get_recommendations(df)
        
        for rec in recommendations:
            st.success(f"‚úÖ {rec}")

def render_history_page():
    st.markdown('<h2 class="section-header">üìö Historique des importations</h2>', unsafe_allow_html=True)
    
    uploads_df = db_manager.get_uploads()
    
    if uploads_df.empty:
        st.info("üì≠ Aucun historique d'importation disponible.")
    else:
        # Affichage avec colonnes personnalis√©es
        st.dataframe(
            uploads_df[['id', 'filename', 'upload_date', 'file_type', 'rows', 'columns']],
            use_container_width=True,
            column_config={
                "id": "ID",
                "filename": "Nom du fichier",
                "upload_date": "Date d'import",
                "file_type": "Type",
                "rows": "Lignes",
                "columns": "Colonnes"
            }
        )
        
        # S√©lection et rechargement
        if len(uploads_df) > 0:
            selected_id = st.selectbox(
                "üîÑ S√©lectionner une importation √† recharger",
                uploads_df['id'].tolist(),
                format_func=lambda x: f"ID {x}: {uploads_df[uploads_df['id']==x]['filename'].iloc[0]}"
            )
            
            if st.button("üîÑ Recharger cette importation", type="primary"):
                # Recharger les donn√©es (simulation)
                selected_row = uploads_df[uploads_df['id'] == selected_id].iloc[0]
                st.session_state['filename'] = selected_row['filename']
                st.success(f"‚úÖ Importation {selected_row['filename']} recharg√©e!")

def render_powerbi_page():
    st.markdown('<h2 class="section-header">‚öôÔ∏è Export PowerBI</h2>', unsafe_allow_html=True)

    if 'data' not in st.session_state:
        st.warning("‚ö†Ô∏è Aucune donn√©e √† exporter. Veuillez d'abord importer un fichier.")
        return

    df = st.session_state['data']
    filename = st.session_state.get('filename', 'data')

    st.markdown("### üéØ Pr√©paration pour PowerBI")

    col1, col2 = st.columns(2)
    with col1:
        export_format = st.selectbox(
            "Format d'export",
            ["Excel (.xlsx)", "CSV", "JSON", "Template PowerBI"]
        )
    with col2:
        include_metadata = st.checkbox("Inclure les m√©tadonn√©es", value=True)

    st.markdown("#### üëÄ Pr√©visualisation des donn√©es √† exporter")
    st.dataframe(df.head(), use_container_width=True)

    if st.button("üöÄ G√©n√©rer l'export PowerBI", type="primary"):
        with st.spinner("‚è≥ G√©n√©ration en cours..."):
            exports = powerbi_exporter.create_powerbi_export(df, filename, include_metadata)

            # üî• Nouveau : Pack ‚ÄúTemplate Power BI‚Äù (PBIDS + ZIP complet)
            pack = powerbi_exporter.create_powerbi_template_package(df, filename, include_metadata)

        st.success("‚úÖ Export g√©n√©r√© avec succ√®s!")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.download_button(
                "üìä T√©l√©charger Excel",
                exports['excel'],
                file_name=f"{filename}_powerbi.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="download_excel_pbi_new"
            )

        with col2:
            st.download_button(
                "üìã T√©l√©charger CSV",
                pack['csv_str'],
                file_name=pack['csv_download_name'],
                mime="text/csv",
                key="download_csv_pbi_new"
            )

        with col3:
            st.download_button(
                "‚öôÔ∏è Template JSON (m√©ta)",
                pack['template_json_str'],
                file_name=f"{filename}_template.json",
                mime="application/json",
                key="download_template_json_new"
            )

        with col4:
            st.download_button(
                "üß© Ouvrir dans Power BI (PBIDS)",
                pack['pbids_str'],
                file_name=pack['pbids_download_name'],
                mime="application/json",
                key="download_pbids_new"
            )

        st.download_button(
            "üì¶ T√©l√©charger le pack complet (ZIP)",
            pack['zip_bytes'],
            file_name=pack['zip_download_name'],
            mime="application/zip",
            use_container_width=True,
            key="download_zip_pack"
        )

        with st.expander("üìñ Instructions d‚Äôimport Power BI (officielles)"):
            st.markdown("""
    **Option rapide (recommand√©e)**  
    1. T√©l√©chargez le **ZIP** et **d√©compressez**.  
    2. Double-cliquez le **`.pbids`** ‚Üí Power BI Desktop s'ouvre avec la source pr√©configur√©e.  
    3. Chargez les donn√©es, cr√©ez vos visuels, puis **Fichier ‚Üí Exporter ‚Üí Mod√®le Power BI (.pbit)**.

    **Rappels utiles :**
    - Le fichier **PBIDS** est un des moyens officiels pour pr√©configurer *Get Data* (une source par fichier).  
    - Le **vrai template** r√©utilisable est le **`.pbit`**, qu‚Äôon **enregistre depuis Power BI Desktop** apr√®s avoir ouvert le PBIDS/CSV.
    """)


if __name__ == "__main__":
    main()